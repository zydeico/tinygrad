{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the docs for tinygrad. This page is for users of the tinygrad library. tinygrad is not 1.0 yet, but it will be soon. The API has been pretty stable for a while.</p> <p>While you can <code>pip install tinygrad</code>, we encourage you to install from source:</p> <pre><code>git clone https://github.com/tinygrad/tinygrad.git\ncd tinygrad\npython3 -m pip install -e .\n</code></pre> <p>After you have installed tinygrad, try the MNIST tutorial</p> <p>We also have developer docs, and Di Zhu has created a bunch of tutorials to help understand how tinygrad works.</p>"},{"location":"#tinygrad-usage","title":"tinygrad Usage","text":"<p>The main class you will interact with is Tensor. It functions very similarly to PyTorch, but has a bit more of a functional style. tinygrad supports many datatypes.  All operations in tinygrad are lazy, meaning they won't do anything until you realize.</p> <ul> <li>tinygrad has a built in neural network library with some classes, optimizers, and load/save state management.</li> <li>tinygrad has a JIT to make things fast. Decorate your pure function with <code>TinyJit</code></li> <li>tinygrad has amazing support for multiple GPUs, allowing you to shard your Tensors with <code>Tensor.shard</code></li> </ul> <p>To understand what training looks like in tinygrad, you should read <code>beautiful_mnist.py</code></p> <p>We have a quickstart guide and a showcase</p>"},{"location":"#differences-from-pytorch","title":"Differences from PyTorch","text":"<p>If you are migrating from PyTorch, welcome. Most of the API is the same. We hope you will find tinygrad both familiar and somehow more \"correct feeling\"</p>"},{"location":"#tinygrad-doesnt-have-nnmodule","title":"tinygrad doesn't have nn.Module","text":"<p>There's nothing special about a \"Module\" class in tinygrad, it's just a normal class. <code>nn.state.get_parameters</code> can be used to recursively search normal classes for valid tensors. Instead of the <code>forward</code> method in PyTorch, tinygrad just uses <code>__call__</code></p>"},{"location":"#tinygrad-is-functional","title":"tinygrad is functional","text":"<p>In tinygrad, you can do <code>x.conv2d(w, b)</code> or <code>x.sparse_categorical_cross_entropy(y)</code>. We do also have a <code>Conv2D</code> class like PyTorch if you want a place to keep the state, but all stateless operations don't have classes.</p>"},{"location":"#tinygrad-is-lazy","title":"tinygrad is lazy","text":"<p>When you do <code>a+b</code> in tinygrad, nothing happens. It's not until you <code>realize</code> the Tensor that the computation actually runs.</p>"},{"location":"#tinygrad-requires-tinyjit-to-be-fast","title":"tinygrad requires @TinyJIT to be fast","text":"<p>PyTorch spends a lot of development effort to make dispatch very fast. tinygrad doesn't. We have a simple decorator that will replay the kernels used in the decorated function.</p>"},{"location":"developer/","title":"Developer","text":"<p>The tinygrad framework has four pieces</p> <ul> <li>a PyTorch like frontend.</li> <li>a scheduler which breaks the compute into kernels.</li> <li>a lowering engine which converts ASTs into code that can run on the accelerator.</li> <li>an execution engine which can run that code.</li> </ul>"},{"location":"developer/#frontend","title":"Frontend","text":"<p>Everything in Tensor is syntactic sugar around function.py, where the forwards and backwards passes are implemented for the different mlops. There's about 25 of them, implemented using about 20 basic ops. Those basic ops go on to construct a graph of:</p> <p>The <code>LazyBuffer</code> graph specifies the compute in terms of low level tinygrad ops. Not all LazyBuffers will actually become realized. There's two types of LazyBuffers, base and view. base contains compute into a contiguous buffer, and view is a view (specified by a ShapeTracker). Inputs to a base can be either base or view, inputs to a view can only be a single base.</p>"},{"location":"developer/#tinygrad.lazy.LazyBuffer","title":"LazyBuffer","text":"<pre><code>LazyBuffer(\n    device: str,\n    st: ShapeTracker,\n    dtype: DType,\n    op: Optional[Op] = None,\n    arg: Any = None,\n    srcs: Tuple[LazyBuffer, ...] = (),\n    base: Optional[LazyBuffer] = None,\n)\n</code></pre>"},{"location":"developer/#scheduling","title":"Scheduling","text":"<p>The scheduler converts the graph of LazyBuffers into a list of <code>ScheduleItem</code>. One <code>ScheduleItem</code> is one kernel on the GPU, and the scheduler is responsible for breaking the large compute graph into subgraphs that can fit in a kernel. <code>ast</code> specifies what compute to run, and <code>bufs</code> specifies what buffers to run it on.</p>"},{"location":"developer/#tinygrad.engine.schedule.ScheduleItem","title":"ScheduleItem  <code>dataclass</code>","text":"<pre><code>ScheduleItem(\n    ast: Tuple[LazyOp, ...], bufs: Tuple[Buffer, ...]\n)\n</code></pre>"},{"location":"developer/#tinygrad.engine.schedule.ScheduleItem.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs: Tuple[Buffer, ...]\n</code></pre> <p>Read only buffers in the schedule.</p>"},{"location":"developer/#tinygrad.engine.schedule.ScheduleItem.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: Tuple[Buffer, ...]\n</code></pre> <p>Read/write or write only buffers in the schedule.</p>"},{"location":"developer/#lowering","title":"Lowering","text":"<p>The code in realize lowers <code>ScheduleItem</code> to <code>ExecItem</code> with</p> <p>There's a ton of complexity hidden behind this, see the <code>codegen/</code> directory.</p> <p>First we lower the AST to UOps, which is a linear list of the compute to be run. This is where the BEAM search happens.</p> <p>Then we render the UOps into code with a <code>Renderer</code>, then we compile the code to binary with a <code>Compiler</code>.</p>"},{"location":"developer/#tinygrad.engine.realize.lower_schedule","title":"lower_schedule","text":"<pre><code>lower_schedule(\n    schedule: List[ScheduleItem],\n) -&gt; Generator[ExecItem, None, None]\n</code></pre> Source code in <code>tinygrad/engine/realize.py</code> <pre><code>def lower_schedule(schedule:List[ScheduleItem]) -&gt; Generator[ExecItem, None, None]:\n  while len(schedule): yield lower_schedule_item(schedule.pop(0))\n</code></pre>"},{"location":"developer/#execution","title":"Execution","text":"<p>Creating <code>ExecItem</code>, which has a run method</p> <p>Lists of <code>ExecItem</code> can be condensed into a single ExecItem with the Graph API (rename to Queue?)</p>"},{"location":"developer/#tinygrad.engine.realize.ExecItem","title":"ExecItem  <code>dataclass</code>","text":"<pre><code>ExecItem(prg: Runner, bufs: List[Optional[Buffer]])\n</code></pre>"},{"location":"developer/#tinygrad.engine.realize.ExecItem.bufs","title":"bufs  <code>instance-attribute</code>","text":"<pre><code>bufs: List[Optional[Buffer]]\n</code></pre>"},{"location":"developer/#tinygrad.engine.realize.ExecItem.prg","title":"prg  <code>instance-attribute</code>","text":"<pre><code>prg: Runner\n</code></pre>"},{"location":"developer/#tinygrad.engine.realize.ExecItem.run","title":"run","text":"<pre><code>run(\n    var_vals: Optional[Dict[Variable, int]] = None,\n    wait=False,\n    jit=False,\n    do_update_stats=True,\n) -&gt; Optional[float]\n</code></pre> Source code in <code>tinygrad/engine/realize.py</code> <pre><code>def run(self, var_vals:Optional[Dict[Variable, int]]=None, wait=False, jit=False, do_update_stats=True) -&gt; Optional[float]:\n  bufs = [cast(Buffer, x) for x in self.bufs] if jit else [cast(Buffer, x).ensure_allocated() for x in self.bufs]\n  et = self.prg(bufs, var_vals if var_vals is not None else {}, wait=wait or DEBUG &gt;= 2)\n  if do_update_stats:\n    GlobalCounters.kernel_count += 1\n    GlobalCounters.global_ops += (op_estimate:=sym_infer(self.prg.op_estimate, var_vals))\n    GlobalCounters.global_mem += (mem_estimate:=sym_infer(self.prg.mem_estimate, var_vals))\n    if et is not None: GlobalCounters.time_sum_s += et\n    if DEBUG &gt;= 2:\n      ptm = (colored(f\"{et*1e3:9.2f}ms\", \"yellow\") if et &gt; 0.01 else f\"{et*1e6:9.2f}us\") if et is not None else \"\"\n      print(f\"{colored(f'*** {self.prg.dname[:7]:7s} {GlobalCounters.kernel_count:4d}', 'magenta' if jit else ('green' if self.prg.first_run else None))} {self.prg.display_name+' '*(38-ansilen(self.prg.display_name))} arg {len(self.bufs):3d} mem {GlobalCounters.mem_used/1e9:5.2f} GB \" +  # noqa: E501\n            (str() if et is None else f\"tm {ptm}/{GlobalCounters.time_sum_s*1e3:9.2f}ms ({op_estimate/((et or 1e-20)*1e9):8.2f} GFLOPS, {mem_estimate/((et or 1e-20)*1e9):7.2f} GB/s)\"))  # noqa: E501\n    self.prg.first_run = False\n  return et\n</code></pre>"},{"location":"dtypes/","title":"dtypes","text":""},{"location":"dtypes/#tinygrad.dtypes","title":"dtypes","text":""},{"location":"dtypes/#tinygrad.dtypes.bfloat16","title":"bfloat16  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bfloat16: Final[DType] = DType(10, 2, '__bf16', None, 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.bool","title":"bool  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bool: Final[DType] = DType(0, 1, 'bool', '?', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.char","title":"char  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>char = int8\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.default_float","title":"default_float  <code>class-attribute</code>","text":"<pre><code>default_float: DType = float32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.default_int","title":"default_int  <code>class-attribute</code>","text":"<pre><code>default_int: DType = int32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.double","title":"double  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>double = float64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.float","title":"float  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>float = float32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.float16","title":"float16  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>float16: Final[DType] = DType(9, 2, 'half', 'e', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.float32","title":"float32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>float32: Final[DType] = DType(11, 4, 'float', 'f', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.float64","title":"float64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>float64: Final[DType] = DType(12, 8, 'double', 'd', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.half","title":"half  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>half = float16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.int","title":"int  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>int = int32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.int16","title":"int16  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>int16: Final[DType] = DType(3, 2, 'short', 'h', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.int32","title":"int32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>int32: Final[DType] = DType(5, 4, 'int', 'i', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.int64","title":"int64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>int64: Final[DType] = DType(7, 8, 'long', 'l', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.int8","title":"int8  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>int8: Final[DType] = DType(1, 1, 'char', 'b', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.long","title":"long  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>long = int64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.short","title":"short  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>short = int16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.uchar","title":"uchar  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uchar = uint8\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.uint","title":"uint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uint = uint32\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.uint16","title":"uint16  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uint16: Final[DType] = DType(4, 2, \"unsigned short\", \"H\", 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.uint32","title":"uint32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uint32: Final[DType] = DType(6, 4, 'unsigned int', 'I', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.uint64","title":"uint64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uint64: Final[DType] = DType(8, 8, 'unsigned long', 'L', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.uint8","title":"uint8  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>uint8: Final[DType] = DType(2, 1, 'unsigned char', 'B', 1)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.ulong","title":"ulong  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ulong = uint64\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.ushort","title":"ushort  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ushort = uint16\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.as_const","title":"as_const  <code>staticmethod</code>","text":"<pre><code>as_const(val: ConstType, dtype: DType)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef as_const(val: ConstType, dtype:DType): return int(val) if dtypes.is_int(dtype) else float(val) if dtypes.is_float(dtype) else bool(val)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.fields","title":"fields  <code>staticmethod</code>","text":"<pre><code>fields() -&gt; Dict[str, DType]\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef fields() -&gt; Dict[str, DType]: return DTYPES_DICT\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.from_np","title":"from_np  <code>staticmethod</code>","text":"<pre><code>from_np(x: type) -&gt; DType\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef from_np(x: type) -&gt; DType: return DTYPES_DICT[np.dtype(x).name]\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.from_py","title":"from_py  <code>staticmethod</code>","text":"<pre><code>from_py(x) -&gt; DType\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod  # NOTE: isinstance(True, int) is True in python\ndef from_py(x) -&gt; DType: return dtypes.default_float if isinstance(x, float) else dtypes.bool if isinstance(x, bool) else dtypes.default_int\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.imagef","title":"imagef  <code>staticmethod</code>","text":"<pre><code>imagef(shp)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef imagef(shp): return ImageDType(100, 4, \"imagef\", 'f', 1, shape=shp, base=dtypes.float32)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.imageh","title":"imageh  <code>staticmethod</code>","text":"<pre><code>imageh(shp)\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef imageh(shp): return ImageDType(100, 2, \"imageh\", 'e', 1, shape=shp, base=dtypes.float32)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.is_float","title":"is_float  <code>staticmethod</code>","text":"<pre><code>is_float(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef is_float(x: DType) -&gt; bool: return x.scalar() in (dtypes.float16, dtypes.bfloat16, dtypes.float32, dtypes.float64)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.is_int","title":"is_int  <code>staticmethod</code>","text":"<pre><code>is_int(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod # static methds on top, or bool in the type info will refer to dtypes.bool\ndef is_int(x: DType) -&gt; bool: return x.scalar() in (dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64) or dtypes.is_unsigned(x)\n</code></pre>"},{"location":"dtypes/#tinygrad.dtypes.is_unsigned","title":"is_unsigned  <code>staticmethod</code>","text":"<pre><code>is_unsigned(x: DType) -&gt; bool\n</code></pre> Source code in <code>tinygrad/dtype.py</code> <pre><code>@staticmethod\ndef is_unsigned(x: DType) -&gt; bool: return x.scalar() in (dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64)\n</code></pre>"},{"location":"env_vars/","title":"List of environment variables that control tinygrad behavior.","text":"<p>This is a list of environment variable that control the runtime behavior of tinygrad and its examples. Most of these are self-explanatory, and are usually used to set an option at runtime.</p> <p>Example: <code>GPU=1 DEBUG=4 python3 -m pytest</code></p> <p>However you can also decorate a function to set a value only inside that function.</p> <pre><code># in tensor.py (probably only useful if you are a tinygrad developer)\n@Context(DEBUG=4)\ndef numpy(self) -&gt; ...\n</code></pre> <p>Or use contextmanager to temporarily set a value inside some scope:</p> <pre><code>with Context(DEBUG=0):\n  a = Tensor.ones(10, 10)\n  a *= 2\n</code></pre>"},{"location":"env_vars/#global-variables","title":"Global Variables","text":"<p>The columns of this list are are: Variable, Possible Value(s) and Description.</p> <ul> <li>A <code>#</code> means that the variable can take any integer value.</li> </ul> <p>These control the behavior of core tinygrad even when used as a library.</p> Variable Possible Value(s) Description DEBUG [1-6] enable debugging output, with 4 you get operations, timings, speed, generated code and more GPU [1] enable the GPU backend CUDA [1] enable CUDA backend HSA [1] enable HSA backend METAL [1] enable Metal backend (for Mac M1 and after) METAL_XCODE [1] enable Metal using macOS Xcode SDK CLANG [1] enable Clang backend LLVM [1] enable LLVM backend BEAM [#] number of beams in kernel beam search GRAPH [1] create a graph of all operations (requires graphviz) GRAPHUOPS [1] create a graph of uops (requires graphviz and saves at /tmp/uops.{svg,dot}) GRAPHPATH [/path/to] where to put the generated graph DEFAULT_FLOAT [HALF, ...] specify the default float dtype (FLOAT32, HALF, BFLOAT16, FLOAT64, ...), default to FLOAT32 IMAGE [1-2] enable 2d specific optimizations FLOAT16 [1] use float16 for images instead of float32 PTX [1] enable the specialized PTX assembler for Nvidia GPUs. If not set, defaults to generic CUDA codegen backend."},{"location":"function/","title":"Function","text":""},{"location":"function/#tinygrad.function","title":"function","text":"<p>This is where the forwards and backwards passes live.</p>"},{"location":"function/#tinygrad.function.dtypes","title":"dtypes","text":""},{"location":"function/#tinygrad.function.DType","title":"DType  <code>dataclass</code>","text":"<pre><code>DType(\n    priority: int,\n    itemsize: int,\n    name: str,\n    fmt: Optional[str],\n    count: int,\n)\n</code></pre>"},{"location":"function/#tinygrad.function.UnaryOps","title":"UnaryOps","text":"<p>               Bases: <code>Enum</code></p> <p>A -&gt; A (elementwise)</p>"},{"location":"function/#tinygrad.function.BinaryOps","title":"BinaryOps","text":"<p>               Bases: <code>Enum</code></p> <p>A + A -&gt; A (elementwise)</p>"},{"location":"function/#tinygrad.function.TernaryOps","title":"TernaryOps","text":"<p>               Bases: <code>Enum</code></p> <p>A + A + A -&gt; A (elementwise)</p>"},{"location":"function/#tinygrad.function.ReduceOps","title":"ReduceOps","text":"<p>               Bases: <code>Enum</code></p> <p>A -&gt; B (reduce)</p>"},{"location":"function/#tinygrad.function.Function","title":"Function","text":"<pre><code>Function(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre>"},{"location":"function/#tinygrad.function.LazyBuffer","title":"LazyBuffer","text":"<pre><code>LazyBuffer(\n    device: str,\n    st: ShapeTracker,\n    dtype: DType,\n    op: Optional[Op] = None,\n    arg: Any = None,\n    srcs: Tuple[LazyBuffer, ...] = (),\n    base: Optional[LazyBuffer] = None,\n)\n</code></pre>"},{"location":"function/#tinygrad.function.Contiguous","title":"Contiguous","text":"<pre><code>Contiguous(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.ContiguousBackward","title":"ContiguousBackward","text":"<pre><code>ContiguousBackward(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Cast","title":"Cast","text":"<pre><code>Cast(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Neg","title":"Neg","text":"<pre><code>Neg(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Reciprocal","title":"Reciprocal","text":"<pre><code>Reciprocal(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Sin","title":"Sin","text":"<pre><code>Sin(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Relu","title":"Relu","text":"<pre><code>Relu(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Log","title":"Log","text":"<pre><code>Log(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Exp","title":"Exp","text":"<pre><code>Exp(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Sqrt","title":"Sqrt","text":"<pre><code>Sqrt(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Sigmoid","title":"Sigmoid","text":"<pre><code>Sigmoid(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Sign","title":"Sign","text":"<pre><code>Sign(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Less","title":"Less","text":"<pre><code>Less(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Eq","title":"Eq","text":"<pre><code>Eq(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Xor","title":"Xor","text":"<pre><code>Xor(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Add","title":"Add","text":"<pre><code>Add(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Sub","title":"Sub","text":"<pre><code>Sub(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Mul","title":"Mul","text":"<pre><code>Mul(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Div","title":"Div","text":"<pre><code>Div(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Where","title":"Where","text":"<pre><code>Where(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Sum","title":"Sum","text":"<pre><code>Sum(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Max","title":"Max","text":"<pre><code>Max(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Expand","title":"Expand","text":"<pre><code>Expand(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Reshape","title":"Reshape","text":"<pre><code>Reshape(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Permute","title":"Permute","text":"<pre><code>Permute(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Pad","title":"Pad","text":"<pre><code>Pad(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Shrink","title":"Shrink","text":"<pre><code>Shrink(\n    device: Union[str, Tuple[str, ...]], *tensors: Tensor\n)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.Flip","title":"Flip","text":"<pre><code>Flip(device: Union[str, Tuple[str, ...]], *tensors: Tensor)\n</code></pre> <p>               Bases: <code>Function</code></p>"},{"location":"function/#tinygrad.function.argsort","title":"argsort","text":"<pre><code>argsort(x)\n</code></pre>"},{"location":"function/#tinygrad.function.sum_acc_dtype","title":"sum_acc_dtype","text":"<pre><code>sum_acc_dtype(dt: DType)\n</code></pre>"},{"location":"mnist/","title":"MNIST Tutorial","text":"<p>After you have installed tinygrad, this is a great first tutorial.</p> <p>Start up a notebook locally, or use colab. tinygrad is very lightweight, so it's easy to install anywhere and doesn't need a special colab image, but for speed we recommend a T4 GPU image.</p>"},{"location":"mnist/#one-liner-to-install-tinygrad-in-colab","title":"One-liner to install tinygrad in colab","text":"<pre><code>!pip install git+https://github.com/tinygrad/tinygrad.git\n</code></pre>"},{"location":"mnist/#whats-the-default-device","title":"What's the default device?","text":"<pre><code>from tinygrad import Device\nprint(Device.DEFAULT)\n</code></pre> <p>You will see <code>CUDA</code> here on a GPU instance, or <code>CLANG</code> here on a CPU instance.</p>"},{"location":"mnist/#a-simple-model","title":"A simple model","text":"<p>We'll use the model from the Keras tutorial.</p> <pre><code>from tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n</code></pre> <p>Two key differences from PyTorch:</p> <ul> <li>Only the stateful layers are declared in <code>__init__</code></li> <li>There's no <code>nn.Module</code> class or <code>forward</code> function, just a normal class and <code>__call__</code></li> </ul>"},{"location":"mnist/#getting-the-dataset","title":"Getting the dataset","text":"<pre><code>from tinygrad.nn.datasets import mnist\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n# (60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar\n</code></pre> <p>tinygrad includes MNIST, it only adds four lines. Free feel to read the function.</p>"},{"location":"mnist/#using-the-model","title":"Using the model","text":"<p>MNIST is small enough that the <code>mnist()</code> function copies the dataset to the default device.</p> <p>So creating the model and evaluating it is a matter of:</p> <pre><code>model = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\n# NOTE: tinygrad is lazy, and hasn't actually run anything by this point\nprint(acc.item())  # ~10% accuracy, as expected from a random model\n</code></pre>"},{"location":"mnist/#training-the-model","title":"Training the model","text":"<p>We'll use the Adam optimizer. The <code>nn.state.get_parameters</code> will walk the model class and pull out the parameters for the optimizer. Also, in tinygrad, it's typical to write a function to do the training step so it can be jitted.</p> <pre><code>optim = nn.optim.Adam(nn.state.get_parameters(model))\nbatch_size = 128\ndef step():\n  Tensor.training = True  # makes dropout work\n  samples = Tensor.randint(batch_size, high=X_train.shape[0])\n  X, Y = X_train[samples], Y_train[samples]\n  optim.zero_grad()\n  loss = model(X).sparse_categorical_crossentropy(Y).backward()\n  optim.step()\n  return loss\n</code></pre> <p>You can time a step with:</p> <pre><code>import timeit\ntimeit.repeat(step, repeat=5, number=1)\n#[0.08268719699981375,\n# 0.07478952900009972,\n# 0.07714716600003158,\n# 0.07785399599970333,\n# 0.07605237000007037]\n</code></pre> <p>So around 75 ms on T4 colab.</p>"},{"location":"mnist/#why-so-slow","title":"Why so slow?","text":"<p>Unlike PyTorch, tinygrad isn't designed to be fast like that. While 75 ms for one step is plenty fast for debugging, it's not great for training. Here, we introduce the first quintessentially tinygrad concept, the <code>TinyJit</code>.</p> <pre><code>from tinygrad import TinyJit\njit_step = TinyJit(step)\n</code></pre> <p>Note</p> <p>It can also be used as a decorator <code>@TinyJit</code></p> <p>Now when we time it:</p> <pre><code>import timeit\ntimeit.repeat(jit_step, repeat=5, number=1)\n# [0.2596786549997887,\n#  0.08989566299987928,\n#  0.0012115650001760514,\n#  0.001010227999813651,\n#  0.0012164899999334011]\n</code></pre> <p>1.0 ms is 75x faster! Note that we aren't syncing the GPU, so GPU time may be slower.</p> <p>The slowness the first two times is the JIT capturing the kernels. And this JIT will not run any Python in the function, it will just replay the tinygrad kernels that were run, so be aware that non tinygrad Python operations won't work. Randomness functions as expected.</p> <p>Unlike other JITs, we JIT everything, including the optimizer. Think of it as a dumb replay on different data.</p>"},{"location":"mnist/#putting-it-together","title":"Putting it together","text":"<p>Since we are just randomly sampling from the dataset, there's no real concept of an epoch. We have a batch size of 128, so the Keras example is taking about 7000 steps.</p> <pre><code>for step in range(7000):\n  loss = jit_step()\n  if step%100 == 0:\n    Tensor.training = False\n    acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n    print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")\n</code></pre> <p>It doesn't take long to reach 98%, and it usually reaches 99%.</p> <pre><code>step    0, loss 3.98, acc 62.53%\nstep  100, loss 0.38, acc 94.25%\nstep  200, loss 0.35, acc 96.46%\nstep  300, loss 0.11, acc 96.70%\nstep  400, loss 0.12, acc 96.81%\nstep  500, loss 0.17, acc 97.43%\nstep  600, loss 0.16, acc 97.34%\nstep  700, loss 0.17, acc 97.52%\nstep  800, loss 0.27, acc 97.59%\nstep  900, loss 0.09, acc 97.62%\nstep 1000, loss 0.16, acc 97.82%\nstep 1100, loss 0.13, acc 97.74%\nstep 1200, loss 0.05, acc 97.79%\nstep 1300, loss 0.17, acc 97.47%\nstep 1400, loss 0.07, acc 97.60%\nstep 1500, loss 0.13, acc 97.99%\nstep 1600, loss 0.12, acc 97.92%\nstep 1700, loss 0.09, acc 98.15%\n...\n</code></pre>"},{"location":"mnist/#from-here","title":"From here?","text":"<p>tinygrad is yours to play with now. It's pure Python and short, so unlike PyTorch, fixing library bugs is well within your abilities.</p> <ul> <li>It's two lines to add multiGPU support to this example (can you find them?). You have to <code>.shard</code> the model to all GPUs, and <code>.shard</code> the dataset by batch.</li> <li><code>with Context(DEBUG=2)</code> shows the running kernels, <code>DEBUG=4</code> shows the code. All <code>Context</code> variables can also be environment variables.</li> <li><code>with Context(BEAM=2)</code> will do a BEAM search on the kernels, searching many possible implementations for what runs the fastest on your hardware. After this search, tinygrad is usually speed competitive with PyTorch, and the results are cached so you won't have to search next time.</li> </ul> <p>Join our Discord for help, and if you want to be a tinygrad developer. Please read the Discord rules when you get there.</p> <p>Follow us on Twitter to keep up with the project.</p>"},{"location":"nn/","title":"Neural Networks","text":""},{"location":"nn/#neural-network-classes","title":"Neural Network classes","text":""},{"location":"nn/#tinygrad.nn.BatchNorm2d","title":"BatchNorm2d","text":"<pre><code>BatchNorm2d(\n    sz: int,\n    eps=1e-05,\n    affine=True,\n    track_running_stats=True,\n    momentum=0.1,\n)\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, sz:int, eps=1e-5, affine=True, track_running_stats=True, momentum=0.1):\n  self.eps, self.track_running_stats, self.momentum = eps, track_running_stats, momentum\n\n  if affine: self.weight, self.bias = Tensor.ones(sz), Tensor.zeros(sz)\n  else: self.weight, self.bias = None, None\n\n  self.running_mean, self.running_var = Tensor.zeros(sz, requires_grad=False), Tensor.ones(sz, requires_grad=False)\n  self.num_batches_tracked = Tensor.zeros(1, requires_grad=False)\n</code></pre>"},{"location":"nn/#tinygrad.nn.Conv1d","title":"Conv1d","text":"<pre><code>Conv1d(\n    in_channels,\n    out_channels,\n    kernel_size,\n    stride=1,\n    padding=0,\n    dilation=1,\n    groups=1,\n    bias=True,\n)\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n  return Conv2d(in_channels, out_channels, (kernel_size,), stride, padding, dilation, groups, bias)\n</code></pre>"},{"location":"nn/#tinygrad.nn.Conv2d","title":"Conv2d","text":"<pre><code>Conv2d(\n    in_channels,\n    out_channels,\n    kernel_size,\n    stride=1,\n    padding=0,\n    dilation=1,\n    groups=1,\n    bias=True,\n)\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n  self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else tuple(kernel_size)\n  self.stride, self.padding, self.dilation, self.groups = stride, padding, dilation, groups\n  self.weight = self.initialize_weight(out_channels, in_channels, groups)\n  bound = 1 / math.sqrt(cast(int, prod(self.weight.shape[1:])))  # weight shape is always ints but mypy cannot tell\n  self.bias = Tensor.uniform(out_channels, low=-bound, high=bound) if bias else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.ConvTranspose2d","title":"ConvTranspose2d","text":"<pre><code>ConvTranspose2d(\n    in_channels,\n    out_channels,\n    kernel_size,\n    stride=1,\n    padding=0,\n    output_padding=0,\n    dilation=1,\n    groups=1,\n    bias=True,\n)\n</code></pre> <p>               Bases: <code>Conv2d</code></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1, groups=1, bias=True):\n  super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n  self.output_padding = output_padding\n</code></pre>"},{"location":"nn/#tinygrad.nn.Linear","title":"Linear","text":"<pre><code>Linear(in_features, out_features, bias=True)\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, in_features, out_features, bias=True):\n  # TODO: is this init good? torch inits to uniform(-1/sqrt(in_features), 1/sqrt(in_features))\n  self.weight = Tensor.kaiming_uniform(out_features, in_features, a=math.sqrt(5))\n  bound = 1 / math.sqrt(in_features)\n  self.bias = Tensor.uniform(out_features, low=-bound, high=bound) if bias else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.GroupNorm","title":"GroupNorm","text":"<pre><code>GroupNorm(\n    num_groups: int,\n    num_channels: int,\n    eps: float = 1e-05,\n    affine: bool = True,\n)\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, num_groups:int, num_channels:int, eps:float=1e-5, affine:bool=True):\n  self.num_groups, self.num_channels, self.eps = num_groups, num_channels, eps\n  self.weight: Optional[Tensor] = Tensor.ones(num_channels) if affine else None\n  self.bias: Optional[Tensor] = Tensor.zeros(num_channels) if affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.InstanceNorm","title":"InstanceNorm","text":"<pre><code>InstanceNorm(\n    num_features: int,\n    eps: float = 1e-05,\n    affine: bool = True,\n)\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, num_features:int, eps:float=1e-5, affine:bool=True):\n  self.num_features, self.eps = num_features, eps\n  self.weight: Optional[Tensor] = Tensor.ones(num_features) if affine else None\n  self.bias: Optional[Tensor] = Tensor.zeros(num_features) if affine else None\n</code></pre>"},{"location":"nn/#tinygrad.nn.LayerNorm","title":"LayerNorm","text":"<pre><code>LayerNorm(\n    normalized_shape: Union[int, Tuple[int, ...]],\n    eps: float = 1e-05,\n    elementwise_affine: bool = True,\n)\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, normalized_shape:Union[int, Tuple[int, ...]], eps:float=1e-5, elementwise_affine:bool=True):\n  self.normalized_shape = (normalized_shape,) if isinstance(normalized_shape, int) else tuple(normalized_shape)\n  self.axis, self.eps, self.elementwise_affine = tuple(-1-i for i in range(len(self.normalized_shape))), eps, elementwise_affine\n  self.weight, self.bias = (Tensor.ones(*self.normalized_shape), Tensor.zeros(*self.normalized_shape)) if elementwise_affine else (None, None)\n</code></pre>"},{"location":"nn/#tinygrad.nn.LayerNorm2d","title":"LayerNorm2d","text":"<pre><code>LayerNorm2d(\n    normalized_shape: Union[int, Tuple[int, ...]],\n    eps: float = 1e-05,\n    elementwise_affine: bool = True,\n)\n</code></pre> <p>               Bases: <code>LayerNorm</code></p> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, normalized_shape:Union[int, Tuple[int, ...]], eps:float=1e-5, elementwise_affine:bool=True):\n  self.normalized_shape = (normalized_shape,) if isinstance(normalized_shape, int) else tuple(normalized_shape)\n  self.axis, self.eps, self.elementwise_affine = tuple(-1-i for i in range(len(self.normalized_shape))), eps, elementwise_affine\n  self.weight, self.bias = (Tensor.ones(*self.normalized_shape), Tensor.zeros(*self.normalized_shape)) if elementwise_affine else (None, None)\n</code></pre>"},{"location":"nn/#tinygrad.nn.Embedding","title":"Embedding","text":"<pre><code>Embedding(vocab_size: int, embed_size: int)\n</code></pre> Source code in <code>tinygrad/nn/__init__.py</code> <pre><code>def __init__(self, vocab_size:int, embed_size:int):\n  self.vocab_sz, self.embed_sz, self.weight = vocab_size, embed_size, Tensor.glorot_uniform(vocab_size, embed_size)\n</code></pre>"},{"location":"nn/#optimizers","title":"Optimizers","text":""},{"location":"nn/#tinygrad.nn.optim.SGD","title":"SGD","text":"<pre><code>SGD(\n    params: List[Tensor],\n    lr=0.001,\n    momentum=0.0,\n    weight_decay=0.0,\n    nesterov=False,\n    classic=False,\n)\n</code></pre> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def SGD(params: List[Tensor], lr=0.001, momentum=0.0, weight_decay=0.0, nesterov=False, classic=False):\n  return LARS(params, lr, momentum, weight_decay, nesterov, classic, tcoef=0.0)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.LARS","title":"LARS","text":"<pre><code>LARS(\n    params: List[Tensor],\n    lr=0.001,\n    momentum=0.9,\n    weight_decay=0.0001,\n    nesterov=False,\n    classic=True,\n    tcoef=0.001,\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def __init__(self, params:List[Tensor], lr=0.001, momentum=0.9, weight_decay=1e-4, nesterov=False, classic=True, tcoef=0.001):\n  super().__init__(params, lr)\n  self.momentum, self.wd, self.nesterov, self.classic, self.tcoef = momentum, weight_decay, nesterov, classic, tcoef\n  self.b = [Tensor.zeros(*t.shape, dtype=t.dtype, device=t.device, requires_grad=False) for t in self.params] if self.momentum else []\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.AdamW","title":"AdamW","text":"<pre><code>AdamW(\n    params: List[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-08,\n    wd=0.01,\n)\n</code></pre> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def AdamW(params: List[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-8, wd=0.01): return LAMB(params, lr, b1, b2, eps, wd, adam=True)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.Adam","title":"Adam","text":"<pre><code>Adam(\n    params: List[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-08,\n)\n</code></pre> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def Adam(params: List[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-8): return LAMB(params, lr, b1, b2, eps, 0.0, adam=True)\n</code></pre>"},{"location":"nn/#tinygrad.nn.optim.LAMB","title":"LAMB","text":"<pre><code>LAMB(\n    params: List[Tensor],\n    lr=0.001,\n    b1=0.9,\n    b2=0.999,\n    eps=1e-06,\n    wd=0.0,\n    adam=False,\n)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> Source code in <code>tinygrad/nn/optim.py</code> <pre><code>def __init__(self, params: List[Tensor], lr=0.001, b1=0.9, b2=0.999, eps=1e-6, wd=0.0, adam=False):\n  super().__init__(params, lr)\n  self.b1, self.b2, self.eps, self.wd, self.adam = b1, b2, eps, wd, adam\n  self.b1_t, self.b2_t = (Tensor([1], dtype=dtypes.float32, device=self.device, requires_grad=False).realize() for _ in [b1, b2])\n  self.m = [Tensor.zeros(*t.shape, dtype=dtypes.float32, device=t.device, requires_grad=False).contiguous() for t in self.params]\n  self.v = [Tensor.zeros(*t.shape, dtype=dtypes.float32, device=t.device, requires_grad=False).contiguous() for t in self.params]\n</code></pre>"},{"location":"nn/#loadsave","title":"Load/Save","text":""},{"location":"nn/#tinygrad.nn.state.safe_load","title":"safe_load","text":"<pre><code>safe_load(fn: Union[Tensor, str]) -&gt; Dict[str, Tensor]\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def safe_load(fn:Union[Tensor,str]) -&gt; Dict[str, Tensor]:\n  t, json_len, metadata = safe_load_metadata(fn)\n  ret = {}\n  for k,v in metadata.items():\n    if k == \"__metadata__\": continue\n    dtype = safe_dtypes[v['dtype']]\n    sz = (v['data_offsets'][1]-v['data_offsets'][0])\n    ret[k] = t[8+json_len+v['data_offsets'][0]:8+json_len+v['data_offsets'][0]+sz].bitcast(dtype).reshape(v['shape'])\n  return ret\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.safe_save","title":"safe_save","text":"<pre><code>safe_save(\n    tensors: Dict[str, Tensor],\n    fn: str,\n    metadata: Optional[Dict[str, Any]] = None,\n)\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def safe_save(tensors:Dict[str, Tensor], fn:str, metadata:Optional[Dict[str, Any]]=None):\n  headers, offset = {}, 0\n  if metadata: headers['__metadata__'] = metadata\n  for k,v in tensors.items():\n    headers[k] = {'dtype': inverse_safe_dtypes[v.dtype], 'shape': list(v.shape), 'data_offsets':[offset, offset+v.nbytes()]}\n    offset += v.nbytes()\n  j = json.dumps(headers, separators=(',', ':'))\n  j += \"\\x20\"*((8-len(j)%8)%8)\n  pathlib.Path(fn).unlink(missing_ok=True)\n  t = Tensor.empty(8+len(j)+offset, dtype=dtypes.uint8, device=f\"disk:{fn}\")\n  t[0:8].bitcast(dtypes.int64).assign([len(j)])\n  t[8:8+len(j)].assign(list(j.encode('utf-8')))\n  for k,v in safe_load(t).items(): v.assign(tensors[k])\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.get_state_dict","title":"get_state_dict","text":"<pre><code>get_state_dict(\n    obj, prefix: str = \"\", tensor_type=Tensor\n) -&gt; Dict[str, Tensor]\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def get_state_dict(obj, prefix:str='', tensor_type=Tensor) -&gt; Dict[str, Tensor]:\n  if isinstance(obj, tensor_type): return {prefix.strip('.'):obj}\n  if hasattr(obj, '_asdict'): return get_state_dict(obj._asdict(), prefix, tensor_type)  # namedtuple\n  if isinstance(obj, OrderedDict): return get_state_dict(dict(obj), prefix, tensor_type)\n  if hasattr(obj, '__dict__'): return get_state_dict(obj.__dict__, prefix, tensor_type)\n  state_dict = {}\n  if isinstance(obj, (list, tuple)):\n    for i,x in enumerate(obj): state_dict.update(get_state_dict(x, f\"{prefix}{str(i)}.\", tensor_type))\n  elif isinstance(obj, dict):\n    for k,v in obj.items(): state_dict.update(get_state_dict(v, f\"{prefix}{str(k)}.\", tensor_type))\n  return state_dict\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters(obj) -&gt; List[Tensor]\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def get_parameters(obj) -&gt; List[Tensor]: return list(get_state_dict(obj).values())\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(\n    model,\n    state_dict: Dict[str, Tensor],\n    strict=True,\n    verbose=True,\n    consume=False,\n) -&gt; None\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def load_state_dict(model, state_dict:Dict[str, Tensor], strict=True, verbose=True, consume=False) -&gt; None:\n  start_mem_used = GlobalCounters.mem_used\n  with Timing(\"loaded weights in \", lambda et_ns: f\", {(GlobalCounters.mem_used-start_mem_used)/1e9:.2f} GB loaded at {(GlobalCounters.mem_used-start_mem_used)/et_ns:.2f} GB/s\"):  # noqa: E501\n    model_state_dict = get_state_dict(model)\n    if DEBUG &gt;= 1 and len(state_dict) &gt; len(model_state_dict):\n      print(\"WARNING: unused weights in state_dict\", sorted(list(state_dict.keys() - model_state_dict.keys())))\n    for k,v in (t := tqdm(model_state_dict.items(), disable=CI or not verbose)):\n      t.set_description(f\"ram used: {GlobalCounters.mem_used/1e9:5.2f} GB, {k:50s}\")\n      if k not in state_dict and not strict:\n        if DEBUG &gt;= 1: print(f\"WARNING: not loading {k}\")\n        continue\n      if isinstance((mlb:=v.lazydata), MultiLazyBuffer):\n        if isinstance(state_dict[k].lazydata, MultiLazyBuffer): v.replace(state_dict[k]).realize()\n        else: v.replace(state_dict[k].shard(mlb.device, mlb.axis)).realize()\n      else: v.replace(state_dict[k].to(v.device)).realize()\n      if consume: del state_dict[k]\n</code></pre>"},{"location":"nn/#tinygrad.nn.state.torch_load","title":"torch_load","text":"<pre><code>torch_load(fn: str) -&gt; Dict[str, Tensor]\n</code></pre> Source code in <code>tinygrad/nn/state.py</code> <pre><code>def torch_load(fn:str) -&gt; Dict[str, Tensor]:\n  t = Tensor.empty(os.stat(fn).st_size, dtype=dtypes.uint8, device=f\"disk:{fn}\")\n\n  offsets: Dict[Union[str, int], int] = {}\n  lens: Dict[Union[str, int], int] = {}\n  def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad=None, backward_hooks=None, metadata=None):\n    #print(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\n    lens[storage[2]] = storage[4] * storage[1].itemsize\n    if storage[2] not in offsets: return None\n    byte_offset = offsets[storage[2]]+storage_offset*storage[1].itemsize\n    ret = t[byte_offset:byte_offset+prod(size)*storage[1].itemsize].bitcast(storage[1])\n\n    # 7 lines to deal with permuted tensors. NOTE: this currently requires reading off the disk\n    shape_strides = [(s, st) for s,st in zip(size, stride) if s != 1]\n    permute_indexes = [len(shape_strides)-1-y for y in argsort([x[1] for x in shape_strides])]\n    if tuple(permute_indexes) != tuple(range(len(permute_indexes))):\n      intermediate_shape = tuple([shape_strides[x][0] for x in argsort(permute_indexes)])\n      assert tuple([shape_strides[i][1] for i in argsort(permute_indexes)]) == strides_for_shape(intermediate_shape), \"nonpermutable strides\"\n      if DEBUG &gt;= 3: print(f\"WARNING: this torch load is slow. CLANG to permute {intermediate_shape} with {permute_indexes}\")\n      assert storage[1] != dtypes.bfloat16, \"can't CLANG permute BF16\"\n      # TODO: find a nice way to support all shapetracker on disktensors\n      # TODO: BUG: a \".realize()\" is needed here for 'GPU=1 python3 test/models/test_efficientnet.py TestEfficientNet.test_car'\n      ret = ret.clang().reshape(intermediate_shape).permute(permute_indexes).realize()\n\n    return ret.reshape(size)\n\n  class Parameter:\n    def __setstate__(self, state): self.tensor = state[0]\n\n  deserialized_objects: Dict[str, Any] = {}\n  intercept = {\"HalfStorage\": dtypes.float16, \"FloatStorage\": dtypes.float32, \"BFloat16Storage\": dtypes.bfloat16, \"IntStorage\": dtypes.int32,\n               \"LongStorage\": dtypes.int64, \"_rebuild_tensor_v2\": _rebuild_tensor_v2, \"FloatTensor\": None, \"Parameter\": Parameter}\n  whitelist = {\"torch\", \"collections\", \"numpy\", \"_codecs\"}  # NOTE: this is not for security, only speed\n  class Dummy: pass\n  class TorchPickle(pickle.Unpickler):\n    def find_class(self, module, name):\n      module_root = module.split(\".\")[0]\n      if module_root not in whitelist:\n        if DEBUG &gt;= 2: print(f\"WARNING: returning Dummy for {module} {name}\")\n        return Dummy\n      return intercept[name] if module_root == \"torch\" else super().find_class(module, name)\n    def persistent_load(self, pid): return deserialized_objects.get(pid, pid)\n\n  if zipfile.is_zipfile(fn):\n    myzip = zipfile.ZipFile(fn, 'r')\n    base_name = myzip.namelist()[0].split('/', 1)[0]\n    for n in myzip.namelist():\n      if n.startswith(f'{base_name}/data/'):\n        with myzip.open(n) as myfile:\n          offsets[n.split(\"/\")[-1]] = myfile._orig_compress_start # type: ignore\n    with myzip.open(f'{base_name}/data.pkl') as myfile:\n      return TorchPickle(myfile).load()\n  elif tarfile.is_tarfile(fn):\n    with tarfile.open(fn, \"r\") as tar:\n      storages_offset = tar.getmember('storages').offset_data\n      f = unwrap(tar.extractfile('storages'))\n      for i in range(TorchPickle(f).load()):  # num_storages\n        (key, _, storage_type), sz = TorchPickle(f).load(), struct.unpack('&lt;q', f.read(8))[0]\n        offsets[key] = storages_offset + f.tell()\n        f.seek(sz*storage_type.itemsize, 1)\n      f = unwrap(tar.extractfile('tensors'))\n      for _ in range(TorchPickle(f).load()):  # num_tensors\n        (key, storage_id, _), ndim, _ = TorchPickle(f).load(), struct.unpack('&lt;i', f.read(4))[0], f.read(4)\n        size, stride = struct.unpack(f'&lt;{ndim}q', f.read(8 * ndim)), struct.unpack(f'&lt;{ndim}q', f.read(8 * ndim))\n        storage_offset = struct.unpack('&lt;q', f.read(8))[0]\n        deserialized_objects[str(key)] = _rebuild_tensor_v2((None, storage_type, storage_id, None, -1), storage_offset, size, stride)\n      return {k:v.tensor if isinstance(v, Parameter) else v for k,v in TorchPickle(unwrap(tar.extractfile('pickle'))).load().items()}\n  else:\n    with open(fn, \"rb\") as f:\n      pkl = TorchPickle(f)\n      _, _, _, rwd, _, ids, base_offset = pkl.load(), pkl.load(), pkl.load(), f.tell(), pkl.load(), pkl.load(), f.tell()\n      for i in ids:\n        offsets[i] = base_offset + 8\n        base_offset += 8 + lens[i]\n      f.seek(rwd)\n      return TorchPickle(f).load()\n</code></pre>"},{"location":"quickstart/","title":"tinygrad Quick Start Guide","text":"<p>This guide assumes no prior knowledge of pytorch or any other deep learning framework, but does assume some basic knowledge of neural networks. It is intended to be a very quick overview of the high level API that tinygrad provides.</p> <p>This guide is also structured as a tutorial which at the end of it you will have a working model that can classify handwritten digits.</p> <p>We need some imports to get started:</p> <pre><code>import numpy as np\nfrom tinygrad.helpers import Timing\n</code></pre>"},{"location":"quickstart/#tensors","title":"Tensors","text":"<p>Tensors are the base data structure in tinygrad. They can be thought of as a multidimensional array of a specific data type. All high level operations in tinygrad operate on these tensors.</p> <p>The tensor class can be imported like so:</p> <pre><code>from tinygrad.tensor import Tensor\n</code></pre> <p>Tensors can be created from an existing data structure like a python list or numpy ndarray:</p> <pre><code>t1 = Tensor([1, 2, 3, 4, 5])\nna = np.array([1, 2, 3, 4, 5])\nt2 = Tensor(na)\n</code></pre> <p>Tensors can also be created using one of the many factory methods:</p> <pre><code>full = Tensor.full(shape=(2, 3), fill_value=5) # create a tensor of shape (2, 3) filled with 5\nzeros = Tensor.zeros(2, 3) # create a tensor of shape (2, 3) filled with 0\nones = Tensor.ones(2, 3) # create a tensor of shape (2, 3) filled with 1\n\nfull_like = Tensor.full_like(full, fill_value=2) # create a tensor of the same shape as `full` filled with 2\nzeros_like = Tensor.zeros_like(full) # create a tensor of the same shape as `full` filled with 0\nones_like = Tensor.ones_like(full) # create a tensor of the same shape as `full` filled with 1\n\neye = Tensor.eye(3) # create a 3x3 identity matrix\narange = Tensor.arange(start=0, stop=10, step=1) # create a tensor of shape (10,) filled with values from 0 to 9\n\nrand = Tensor.rand(2, 3) # create a tensor of shape (2, 3) filled with random values from a uniform distribution\nrandn = Tensor.randn(2, 3) # create a tensor of shape (2, 3) filled with random values from a normal distribution\nuniform = Tensor.uniform(2, 3, low=0, high=10) # create a tensor of shape (2, 3) filled with random values from a uniform distribution between 0 and 10\n</code></pre> <p>There are even more of these factory methods, you can find them in the Tensor file.</p> <p>All the tensors creation methods can take a <code>dtype</code> argument to specify the data type of the tensor.</p> <pre><code>from tinygrad.dtype import dtypes\n\nt3 = Tensor([1, 2, 3, 4, 5], dtype=dtypes.int32)\n</code></pre> <p>Tensors allow you to perform operations on them like so:</p> <pre><code>t4 = Tensor([1, 2, 3, 4, 5])\nt5 = (t4 + 1) * 2\nt6 = (t5 * t4).relu().log_softmax()\n</code></pre> <p>All of these operations are lazy and are only executed when you realize the tensor using <code>.realize()</code> or <code>.numpy()</code>.</p> <pre><code>print(t6.numpy())\n# [-56. -48. -36. -20.   0.]\n</code></pre> <p>There are a lot more operations that can be performed on tensors, you can find them in the Tensor file. Additionally reading through abstractions2.py will help you understand how operations on these tensors make their way down to your hardware.</p>"},{"location":"quickstart/#models","title":"Models","text":"<p>Neural networks in tinygrad are really just represented by the operations performed on tensors. These operations are commonly grouped into the <code>__call__</code> method of a class which allows modularization and reuse of these groups of operations. These classes do not need to inherit from any base class, in fact if they don't need any trainable parameters they don't even need to be a class!</p> <p>An example of this would be the <code>nn.Linear</code> class which represents a linear layer in a neural network.</p> <pre><code>class Linear:\n  def __init__(self, in_features, out_features, bias=True, initialization: str='kaiming_uniform'):\n    self.weight = getattr(Tensor, initialization)(out_features, in_features)\n    self.bias = Tensor.zeros(out_features) if bias else None\n\n  def __call__(self, x):\n    return x.linear(self.weight.transpose(), self.bias)\n</code></pre> <p>There are more neural network modules already implemented in nn, and you can also implement your own.</p> <p>We will be implementing a simple neural network that can classify handwritten digits from the MNIST dataset. Our classifier will be a simple 2 layer neural network with a Leaky ReLU activation function. It will use a hidden layer size of 128 and an output layer size of 10 (one for each digit) with no bias on either Linear layer.</p> <pre><code>class TinyNet:\n  def __init__(self):\n    self.l1 = Linear(784, 128, bias=False)\n    self.l2 = Linear(128, 10, bias=False)\n\n  def __call__(self, x):\n    x = self.l1(x)\n    x = x.leakyrelu()\n    x = self.l2(x)\n    return x\n\nnet = TinyNet()\n</code></pre> <p>We can see that the forward pass of our neural network is just the sequence of operations performed on the input tensor <code>x</code>. We can also see that functional operations like <code>leakyrelu</code> are not defined as classes and instead are just methods we can just call. Finally, we just initialize an instance of our neural network, and we are ready to start training it.</p>"},{"location":"quickstart/#training","title":"Training","text":"<p>Now that we have our neural network defined we can start training it. Training neural networks in tinygrad is super simple. All we need to do is define our neural network, define our loss function, and then call <code>.backward()</code> on the loss function to compute the gradients. They can then be used to update the parameters of our neural network using one of the many Optimizers.</p> <p>For our loss function we will be using sparse categorical cross entropy loss. The implementation below is taken from tensor.py, it's copied below to highlight an important detail of tinygrad.</p> <pre><code>def sparse_categorical_crossentropy(self, Y, ignore_index=-1) -&gt; Tensor:\n    loss_mask = Y != ignore_index\n    y_counter = Tensor.arange(self.shape[-1], dtype=dtypes.int32, requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n    y = ((y_counter == Y.flatten().reshape(-1, 1)).where(-1.0, 0) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n    return self.log_softmax().mul(y).sum() / loss_mask.sum()\n</code></pre> <p>As we can see in this implementation of cross entropy loss, there are certain operations that tinygrad does not support natively. Namely, operations that are load/store or assigning a value to a tensor at a certain index. Load/store ops are not supported in tinygrad natively because they add complexity when trying to port to different backends, 90% of the models out there don't use/need them, and they can be implemented like it's done above with an <code>arange</code> mask.</p> <p>For our optimizer we will be using the traditional stochastic gradient descent optimizer with a learning rate of 3e-4.</p> <pre><code>from tinygrad.nn.optim import SGD\n\nopt = SGD([net.l1.weight, net.l2.weight], lr=3e-4)\n</code></pre> <p>We can see that we are passing in the parameters of our neural network to the optimizer. This is due to the fact that the optimizer needs to know which parameters to update. There is a simpler way to do this just by using <code>get_parameters(net)</code> from <code>tinygrad.nn.state</code> which will return a list of all the parameters in the neural network. The parameters are just listed out explicitly here for clarity.</p> <p>Now that we have our network, loss function, and optimizer defined all we are missing is the data to train on! There are a couple of dataset loaders in tinygrad located in /extra/datasets. We will be using the MNIST dataset loader.</p> <pre><code>from extra.datasets import fetch_mnist\n</code></pre> <p>Now we have everything we need to start training our neural network. We will be training for 1000 steps with a batch size of 64.</p> <p>We use <code>with Tensor.train()</code> set the internal flag <code>Tensor.training</code> to <code>True</code> during training. Upon exit, the flag is restored to its previous value by the context manager.</p> <pre><code>X_train, Y_train, X_test, Y_test = fetch_mnist()\n\nwith Tensor.train():\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_train.shape[0], size=(64))\n    batch = Tensor(X_train[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Tensor(Y_train[samp])\n\n    # forward pass\n    out = net(batch)\n\n    # compute loss\n    loss = sparse_categorical_crossentropy(out, labels)\n\n    # zero gradients\n    opt.zero_grad()\n\n    # backward pass\n    loss.backward()\n\n    # update parameters\n    opt.step()\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1)\n    acc = (pred == labels).mean()\n\n    if step % 100 == 0:\n      print(f\"Step {step+1} | Loss: {loss.numpy()} | Accuracy: {acc.numpy()}\")\n</code></pre>"},{"location":"quickstart/#evaluation","title":"Evaluation","text":"<p>Now that we have trained our neural network we can evaluate it on the test set. We will be using the same batch size of 64 and will be evaluating for 1000 of those batches.</p> <pre><code>with Timing(\"Time: \"):\n  avg_acc = 0\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_test.shape[0], size=(64))\n    batch = Tensor(X_test[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Y_test[samp]\n\n    # forward pass\n    out = net(batch)\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1).numpy()\n    avg_acc += (pred == labels).mean()\n  print(f\"Test Accuracy: {avg_acc / 1000}\")\n</code></pre>"},{"location":"quickstart/#and-thats-it","title":"And that's it","text":"<p>Highly recommend you check out the examples/ folder for more examples of using tinygrad. Reading the source code of tinygrad is also a great way to learn how it works. Specifically the tests in test/ are a great place to see how to use and the semantics of the different operations. There are also a bunch of models implemented in models/ that you can use as a reference.</p> <p>Additionally, feel free to ask questions in the <code>#learn-tinygrad</code> channel on the discord. Don't ask to ask, just ask!</p>"},{"location":"quickstart/#extras","title":"Extras","text":""},{"location":"quickstart/#jit","title":"JIT","text":"<p>Additionally, it is possible to speed up the computation of certain neural networks by using the JIT. Currently, this does not support models with varying input sizes and non tinygrad operations.</p> <p>To use the JIT we just need to add a function decorator to the forward pass of our neural network and ensure that the input and output are realized tensors. Or in this case we will create a wrapper function and decorate the wrapper function to speed up the evaluation of our neural network.</p> <pre><code>from tinygrad import TinyJit\n\n@TinyJit\ndef jit(x):\n  return net(x).realize()\n\nwith Timing(\"Time: \"):\n  avg_acc = 0\n  for step in range(1000):\n    # random sample a batch\n    samp = np.random.randint(0, X_test.shape[0], size=(64))\n    batch = Tensor(X_test[samp], requires_grad=False)\n    # get the corresponding labels\n    labels = Y_test[samp]\n\n    # forward pass with jit\n    out = jit(batch)\n\n    # calculate accuracy\n    pred = out.argmax(axis=-1).numpy()\n    avg_acc += (pred == labels).mean()\n  print(f\"Test Accuracy: {avg_acc / 1000}\")\n</code></pre> <p>You will find that the evaluation time is much faster than before and that your accelerator utilization is much higher.</p>"},{"location":"quickstart/#saving-and-loading-models","title":"Saving and Loading Models","text":"<p>The standard weight format for tinygrad is safetensors. This means that you can load the weights of any model also using safetensors into tinygrad. There are functions in state.py to save and load models to and from this format.</p> <pre><code>from tinygrad.nn.state import safe_save, safe_load, get_state_dict, load_state_dict\n\n# first we need the state dict of our model\nstate_dict = get_state_dict(net)\n\n# then we can just save it to a file\nsafe_save(state_dict, \"model.safetensors\")\n\n# and load it back in\nstate_dict = safe_load(\"model.safetensors\")\nload_state_dict(net, state_dict)\n</code></pre> <p>Many of the models in the models/ folder have a <code>load_from_pretrained</code> method that will download and load the weights for you. These usually are pytorch weights meaning that you would need pytorch installed to load them.</p>"},{"location":"quickstart/#environment-variables","title":"Environment Variables","text":"<p>There exist a bunch of environment variables that control the runtime behavior of tinygrad. Some of the commons ones are <code>DEBUG</code> and the different backend enablement variables.</p> <p>You can find a full list and their descriptions in env_vars.md.</p>"},{"location":"quickstart/#visualizing-the-computation-graph","title":"Visualizing the Computation Graph","text":"<p>It is possible to visualize the computation graph of a neural network using graphviz.</p> <p>This is easily done by running a single pass (forward or backward!) of the neural network with the environment variable <code>GRAPH</code> set to <code>1</code>. The graph will be saved to <code>/tmp/net.svg</code> by default.</p>"},{"location":"showcase/","title":"tinygrad Showcase","text":"<p>Despite being a tiny library, tinygrad is capable of doing a lot of things. From state-of-the-art vision to state-of-the-art language models.</p>"},{"location":"showcase/#vision","title":"Vision","text":""},{"location":"showcase/#efficientnet","title":"EfficientNet","text":"<p>You can either pass in the URL of a picture to discover what it is: <pre><code>python3 examples/efficientnet.py ./test/models/efficientnet/Chicken.jpg\n</code></pre> Or, if you have a camera and OpenCV installed, you can detect what is in front of you: <pre><code>python3 examples/efficientnet.py webcam\n</code></pre></p>"},{"location":"showcase/#yolov8","title":"YOLOv8","text":"<p>Take a look at yolov8.py.</p> <p></p>"},{"location":"showcase/#audio","title":"Audio","text":""},{"location":"showcase/#whisper","title":"Whisper","text":"<p>Take a look at whisper.py. You need pyaudio and torchaudio installed.</p> <pre><code>SMALL=1 python3 examples/whisper.py\n</code></pre>"},{"location":"showcase/#generative","title":"Generative","text":""},{"location":"showcase/#generative-adversarial-networks","title":"Generative Adversarial Networks","text":"<p>Take a look at mnist_gan.py.</p> <p></p>"},{"location":"showcase/#stable-diffusion","title":"Stable Diffusion","text":"<pre><code>python3 examples/stable_diffusion.py\n</code></pre> <p>\"a horse sized cat eating a bagel\"</p>"},{"location":"showcase/#llama","title":"LLaMA","text":"<p>You will need to download and put the weights into the <code>weights/LLaMA</code> directory, which may need to be created.</p> <p>Then you can have a chat with Stacy: <pre><code>python3 examples/llama.py\n</code></pre></p>"},{"location":"tensor/","title":"Tensor","text":""},{"location":"tensor/#tinygrad.Tensor","title":"Tensor","text":"<pre><code>Tensor(\n    data: Union[\n        None,\n        ConstType,\n        List,\n        Tuple,\n        LazyBuffer,\n        ndarray,\n        bytes,\n        MultiLazyBuffer,\n        Variable,\n    ],\n    device: Optional[Union[str, tuple, list]] = None,\n    dtype: Optional[DType] = None,\n    requires_grad: Optional[bool] = None,\n)\n</code></pre> <p>A <code>Tensor</code> is a multi-dimensional matrix containing elements of a single data type.</p> <p></p>"},{"location":"tensor/#properties","title":"Properties","text":""},{"location":"tensor/#tinygrad.Tensor.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: Tuple[sint, ...]\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: DType\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.device","title":"device  <code>property</code>","text":"<pre><code>device: Union[str, Tuple[str, ...]]\n</code></pre>"},{"location":"tensor/#tinygrad-ops","title":"tinygrad ops","text":""},{"location":"tensor/#tinygrad.Tensor.schedule_with_vars","title":"schedule_with_vars","text":"<pre><code>schedule_with_vars(\n    *lst: Tensor, seen: Optional[Set[LazyBuffer]] = None\n) -&gt; Tuple[List[ScheduleItem], Dict[Variable, int]]\n</code></pre> <p>Create the schedule needed to realize these Tensor(s), with Variables.</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def schedule_with_vars(self, *lst:Tensor, seen:Optional[Set[LazyBuffer]]=None) -&gt; Tuple[List[ScheduleItem], Dict[Variable, int]]:\n  \"\"\"Create the schedule needed to realize these Tensor(s), with Variables.\"\"\"\n  if getenv(\"FUZZ_SCHEDULE\"):\n    from test.external.fuzz_schedule import fuzz_schedule\n    fuzz_schedule(flatten([x.lazydata.lbs for x in (self,)+lst]))\n  schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]), seen)\n  return memory_planner(schedule), var_vals\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.schedule","title":"schedule","text":"<pre><code>schedule(\n    *lst: Tensor, seen: Optional[Set[LazyBuffer]] = None\n) -&gt; List[ScheduleItem]\n</code></pre> <p>Create the schedule needed to realize these Tensor(s).</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def schedule(self, *lst:Tensor, seen:Optional[Set[LazyBuffer]]=None) -&gt; List[ScheduleItem]:\n  \"\"\"Create the schedule needed to realize these Tensor(s).\"\"\"\n  schedule, var_vals = self.schedule_with_vars(*lst, seen=seen)\n  assert len(var_vals) == 0\n  return schedule\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.realize","title":"realize","text":"<pre><code>realize(*lst: Tensor, do_update_stats=True) -&gt; Tensor\n</code></pre> <p>Trigger the computation needed to create these Tensor(s).</p> Source code in <code>tinygrad/tensor.py</code> <pre><code>def realize(self, *lst:Tensor, do_update_stats=True) -&gt; Tensor:\n  \"\"\"Trigger the computation needed to create these Tensor(s).\"\"\"\n  run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)\n  return self\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.replace","title":"replace","text":"<pre><code>replace(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def replace(self, x:Tensor) -&gt; Tensor:\n  # used for replacing a Tensor with a new version of it (potentially with a different device and dtype)\n  assert not x.requires_grad and getattr(self, '_ctx', None) is None\n  assert self.shape == x.shape, f\"replace shape mismatch {self.shape} != {x.shape}\"\n  self.lazydata = x.lazydata\n  return self\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.assign","title":"assign","text":"<pre><code>assign(x) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def assign(self, x) -&gt; Tensor:\n  # TODO: this is a hack for writing to DISK. remove with working assign\n  if isinstance(self.device, str) and self.device.startswith(\"DISK\"):\n    if x.__class__ is not Tensor: x = Tensor(x, device=\"NPY\", dtype=self.dtype)\n    self.contiguous().realize().lazydata.base.realized.copyin(x.numpy().data)\n    return self\n  if x.__class__ is not Tensor: x = Tensor(x, device=self.device, dtype=self.dtype)\n  if DEBUG &gt;= 4: print(f\"assign {self.lazydata} &lt;- {x.lazydata}\")\n  if self.lazydata is x.lazydata: return self  # a self assign is a NOOP\n  # NOTE: we allow cross device assign\n  assert self.shape == x.shape, f\"assign shape mismatch {self.shape} != {x.shape}\"\n  assert self.device == x.device, f\"assign device mismatch {self.device} != {x.device}\"\n  assert self.dtype == x.dtype, f\"assign dtype mismatch {self.dtype} != {x.dtype}\"\n  assert not isinstance(self.lazydata, MultiLazyBuffer) or self.lazydata.axis == x.lazydata.axis, \"axis must match on MultiLazyBuffer\"\n  assert not x.requires_grad  # self requires_grad is okay?\n  if not self.lazydata.is_realized(): return self.replace(x)\n  self.lazydata = self.lazydata.assign(x.lazydata)\n  return self\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.detach","title":"detach","text":"<pre><code>detach() -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def detach(self) -&gt; Tensor: return Tensor(self.lazydata, device=self.device, requires_grad=False)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.to","title":"to","text":"<pre><code>to(device: Optional[Union[str, Tuple[str, ...]]]) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def to(self, device:Optional[Union[str, Tuple[str, ...]]]) -&gt; Tensor:\n  device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)\n  if device == self.device: return self\n  if not isinstance(device, str): return self.shard(device)\n  ret = Tensor(self.lazydata, device, requires_grad=self.requires_grad)\n  if self.grad is not None: ret.grad = self.grad.to(device)\n  if hasattr(self, '_ctx'): ret._ctx = self._ctx\n  return ret\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.to_","title":"to_","text":"<pre><code>to_(device: Optional[Union[str, Tuple[str, ...]]])\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def to_(self, device:Optional[Union[str, Tuple[str, ...]]]):\n  real = self.to(device)\n  # TODO: is this assign?\n  if self.grad is not None and real.grad is not None: self.grad.lazydata = real.grad.lazydata\n  self.lazydata = real.lazydata\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.shard","title":"shard","text":"<pre><code>shard(\n    devices: Tuple[str, ...], axis: Optional[int] = None\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shard(self, devices:Tuple[str, ...], axis:Optional[int]=None) -&gt; Tensor:\n  assert isinstance(self.lazydata, LazyBuffer), \"can't shard a MultiLazyBuffer\"\n  canonical_devices = tuple(Device.canonicalize(x) for x in devices)\n  if axis is not None and axis &lt; 0: axis += len(self.shape)\n  return Tensor(MultiLazyBuffer.from_sharded(self.lazydata, canonical_devices, axis), device=canonical_devices, requires_grad=self.requires_grad)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.shard_","title":"shard_","text":"<pre><code>shard_(\n    devices: Tuple[str, ...], axis: Optional[int] = None\n)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shard_(self, devices:Tuple[str, ...], axis:Optional[int]=None):\n  self.lazydata = self.shard(devices, axis).lazydata\n  return self\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.contiguous","title":"contiguous","text":"<pre><code>contiguous()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def contiguous(self): return F.Contiguous.apply(self)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.contiguous_backward","title":"contiguous_backward","text":"<pre><code>contiguous_backward()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def contiguous_backward(self): return F.ContiguousBackward.apply(self)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.backward","title":"backward","text":"<pre><code>backward() -&gt; Tensor\n</code></pre> <p>Propagates the gradient of a tensor backwards through the computation graph. Must be used on a scalar tensor.</p> <pre><code>t = Tensor.arange(6.0, requires_grad=True)\nt2 = t.sum()\nt2.backward()\nprint(t.grad.numpy())\n</code></pre> <pre><code>[1. 1. 1. 1. 1. 1.]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def backward(self) -&gt; Tensor:\n  \"\"\"\n  Propagates the gradient of a tensor backwards through the computation graph.\n  Must be used on a scalar tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6.0, requires_grad=True)\n  t2 = t.sum()\n  t2.backward()\n  print(t.grad.numpy())\n  ```\n  \"\"\"\n  assert self.shape == tuple(), f\"backward can only be called for scalar tensors, but it has shape {self.shape})\"\n\n  # fill in the first grad with one. don't use Tensor.ones because we don't need contiguous\n  # this is \"implicit gradient creation\"\n  self.grad = Tensor(1.0, dtype=self.dtype, device=self.device, requires_grad=False)\n\n  for t0 in reversed(self._deepwalk()):\n    if t0.grad is None: raise RuntimeError(f\"tensor {t0} has no grad\")\n    grads = t0._ctx.backward(t0.grad.lazydata)\n    grads = [Tensor(g, device=self.device, requires_grad=False) if g is not None else None\n      for g in ([grads] if len(t0._ctx.parents) == 1 else grads)]\n    for t, g in zip(t0._ctx.parents, grads):\n      if g is not None and t.requires_grad:\n        assert g.shape == t.shape, f\"grad shape must match tensor shape, {g.shape!r} != {t.shape!r}\"\n        t.grad = g if t.grad is None else (t.grad + g)\n    del t0._ctx\n  return self\n</code></pre>"},{"location":"tensor/#creation-basic","title":"Creation (basic)","text":""},{"location":"tensor/#tinygrad.Tensor.empty","title":"empty  <code>staticmethod</code>","text":"<pre><code>empty(*shape, **kwargs)\n</code></pre> <p>Creates an empty tensor with the given shape.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.empty(2, 3)\nprint(t.shape)\n</code></pre> <pre><code>(2, 3)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef empty(*shape, **kwargs):\n  \"\"\"\n  Creates an empty tensor with the given shape.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.empty(2, 3)\n  print(t.shape)\n  ```\n  \"\"\"\n  return Tensor._loadop(LoadOps.EMPTY, argfix(*shape), **kwargs)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.zeros","title":"zeros  <code>staticmethod</code>","text":"<pre><code>zeros(*shape, **kwargs)\n</code></pre> <p>Creates a tensor with the given shape, filled with zeros.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.zeros(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0. 0. 0.]\n [0. 0. 0.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef zeros(*shape, **kwargs):\n  \"\"\"\n  Creates a tensor with the given shape, filled with zeros.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.zeros(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return Tensor.full(argfix(*shape), 0.0, **kwargs)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.ones","title":"ones  <code>staticmethod</code>","text":"<pre><code>ones(*shape, **kwargs)\n</code></pre> <p>Creates a tensor with the given shape, filled with ones.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.ones(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef ones(*shape, **kwargs):\n  \"\"\"\n  Creates a tensor with the given shape, filled with ones.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.ones(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return Tensor.full(argfix(*shape), 1.0, **kwargs)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.full","title":"full  <code>staticmethod</code>","text":"<pre><code>full(\n    shape: Tuple[sint, ...], fill_value: ConstType, **kwargs\n)\n</code></pre> <p>Creates a tensor with the given shape, filled with the given value.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.full((2, 3), 42)\nprint(t.numpy())\n</code></pre> <pre><code>[[42 42 42]\n [42 42 42]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef full(shape:Tuple[sint, ...], fill_value:ConstType, **kwargs):\n  \"\"\"\n  Creates a tensor with the given shape, filled with the given value.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.full((2, 3), 42)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return Tensor(fill_value, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.arange","title":"arange  <code>staticmethod</code>","text":"<pre><code>arange(start, stop=None, step=1, **kwargs)\n</code></pre> <p>If <code>stop</code> is not specified, creates a tensor with the given shape, filled with values from <code>0</code> to <code>start</code> with the given step size.</p> <p>If <code>stop</code> is specified, creates a tensor with the given shape, filled with values from <code>start</code> to <code>stop</code> with the given step size.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.arange(5)\nprint(t.numpy())\n</code></pre> <pre><code>[0 1 2 3 4]\n</code></pre> <pre><code>t = Tensor.arange(5, 10)\nprint(t.numpy())\n</code></pre> <pre><code>[5 6 7 8 9]\n</code></pre> <pre><code>t = Tensor.arange(5, 10, 2)\nprint(t.numpy())\n</code></pre> <pre><code>[5 7 9]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef arange(start, stop=None, step=1, **kwargs):\n  \"\"\"\n  If `stop` is not specified, creates a tensor with the given shape, filled with values from `0` to `start` with the given step size.\n\n  If `stop` is specified, creates a tensor with the given shape, filled with values from `start` to `stop` with the given step size.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(5)\n  print(t.numpy())\n  ```\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(5, 10)\n  print(t.numpy())\n  ```\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(5, 10, 2)\n  print(t.numpy())\n  ```\n  \"\"\"\n  if stop is None: stop, start = start, 0\n  assert all(isinstance(s, (int, float)) for s in (start, stop, step)), f\"symbolic arange not supported {start=}, {stop=}, {step=}\"\n  dtype = kwargs.pop(\"dtype\", dtypes.default_float if any(isinstance(x, float) for x in (start, stop, step)) else dtypes.default_int)\n  return (Tensor.full((math.ceil((stop-start)/step),), step, dtype=dtype, **kwargs)._cumsum() + (start - step)).cast(dtype)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.eye","title":"eye  <code>staticmethod</code>","text":"<pre><code>eye(dim: int, **kwargs)\n</code></pre> <p>Creates an identity matrix of the given dimension.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>t = Tensor.eye(3)\nprint(t.numpy())\n</code></pre> <pre><code>[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef eye(dim:int, **kwargs):\n  \"\"\"\n  Creates an identity matrix of the given dimension.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.eye(3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return Tensor.ones((dim,1),**kwargs).pad((None,(0,dim))).flatten().shrink(((0,dim*dim),)).reshape(dim, dim)\n</code></pre>"},{"location":"tensor/#creation-random","title":"Creation (random)","text":""},{"location":"tensor/#tinygrad.Tensor.rand","title":"rand  <code>staticmethod</code>","text":"<pre><code>rand(\n    *shape,\n    device: Optional[Union[Tuple[str, ...], str]] = None,\n    dtype: Optional[DType] = None,\n    **kwargs\n)\n</code></pre> <p>Creates a tensor with the given shape, filled with random values between the interval <code>[0, 1)</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.rand(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[0.5053239  0.6522992  0.4013064 ]\n [0.04377532 0.57715255 0.02002954]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef rand(*shape, device:Optional[Union[Tuple[str, ...], str]]=None, dtype:Optional[DType]=None, **kwargs):\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values between the interval `[0, 1)`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.rand(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  if Tensor._rng_counter is None: Tensor._rng_counter = Tensor([0], dtype=dtypes.uint32, requires_grad=False)\n  if not THREEFRY.value:\n    # for bfloat16, numpy rand passes buffer in float\n    if (dtype or dtypes.default_float) == dtypes.bfloat16:\n      return Tensor.rand(*shape, **kwargs, device=device, dtype=dtypes.float).cast(dtypes.bfloat16)\n    return Tensor._loadop(LoadOps.CUSTOM, argfix(*shape), arg=custom_random, device=device, dtype=dtype, **kwargs)\n\n  # threefry\n  if (num := prod((shape:=argfix(*shape)))) == 0: return Tensor.zeros(shape, device=device, dtype=dtype, **kwargs)\n  counts1 = (Tensor.arange(math.ceil(num / 2), device=device, dtype=dtypes.uint32, requires_grad=False)+Tensor._rng_counter.to(device)).realize()\n  counts2 = counts1 + math.ceil(num / 2)\n  Tensor._rng_counter.assign(Tensor._rng_counter + num).realize()\n\n  rotations = [[13, 15, 26, 6], [17, 29, 16, 24]]\n  ks = [0x0, Tensor._seed ^ 0x0 ^ 0x1BD11BDA, Tensor._seed]\n\n  x = [counts1 + ks[-1], counts2 + ks[0]]\n  for i in range(5):\n    for r in rotations[i % 2]: x[0], x[1] = (x0 := x[0] + x[1]), x0 ^ ((x[1] &lt;&lt; r) + (x[1] &gt;&gt; (32 - r)))\n    x = [(x[0] + ks[i % 3]), (x[1] + ks[(i + 1) % 3] + i + 1)]\n  out = x[0].cat(x[1]).rshift(8).cast(dtypes.float32).div(2 ** 24)[:num]\n  out = out.reshape(shape).cast(dtypes.default_float if dtype is None else dtype)\n  out.requires_grad = kwargs.get(\"requires_grad\")\n  return out.contiguous()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.randn","title":"randn  <code>staticmethod</code>","text":"<pre><code>randn(\n    *shape, dtype: Optional[DType] = None, **kwargs\n) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a normal distribution with mean <code>0</code> and standard deviation <code>1</code>. If <code>dtype</code> is not specified, the default type is used.</p> <p>You can pass in the <code>device</code> keyword argument to control device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[-0.8042354  -1.1013222  -0.90952474]\n [ 1.2801557  -2.288294    0.70781666]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef randn(*shape, dtype:Optional[DType]=None, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a normal distribution with mean `0` and standard deviation `1`.\n  If `dtype` is not specified, the default type is used.\n\n  You can pass in the `device` keyword argument to control device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  # https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform\n  src = Tensor.rand((2, *argfix(*shape)), **{**kwargs, \"dtype\": dtypes.float32})\n  return src[0].mul(2*math.pi).cos().mul((1 - src[1]).log().mul(-2).sqrt()).cast(dtype or dtypes.default_float)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.normal","title":"normal  <code>staticmethod</code>","text":"<pre><code>normal(*shape, mean=0.0, std=1.0, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a normal distribution with the given mean and standard deviation.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.normal(2, 3, mean=10, std=2)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 8.391529   7.7973557  8.18095  ]\n [12.560311   5.423412  11.415633 ]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef normal(*shape, mean=0.0, std=1.0, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a normal distribution with the given mean and standard deviation.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.normal(2, 3, mean=10, std=2)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return (std * Tensor.randn(*shape, **kwargs)) + mean\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.uniform","title":"uniform  <code>staticmethod</code>","text":"<pre><code>uniform(*shape, low=0.0, high=1.0, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a uniform distribution with the given lower and upper bounds.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.uniform(2, 3, low=2, high=10)\nprint(t.numpy())\n</code></pre> <pre><code>[[6.042591  7.218394  5.210451 ]\n [2.3502026 6.6172204 2.1602364]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef uniform(*shape, low=0.0, high=1.0, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values from a uniform distribution with the given lower and upper bounds.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.uniform(2, 3, low=2, high=10)\n  print(t.numpy())\n  ```\n  \"\"\"\n  dtype = kwargs.pop(\"dtype\", dtypes.default_float)\n  return ((high-low) * Tensor.rand(*shape, **kwargs)).cast(dtype) + low\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.scaled_uniform","title":"scaled_uniform  <code>staticmethod</code>","text":"<pre><code>scaled_uniform(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>Creates a tensor with the given shape, filled with random values from a uniform distribution with a mean of zero and a standard deviation of <code>(prod(shape)**-0.5</code>.</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.scaled_uniform(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.00434694  0.1243518  -0.080583  ]\n [-0.3725059   0.06299479 -0.39189425]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef scaled_uniform(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  Creates a tensor with the given shape, filled with random values\n  from a uniform distribution with a mean of zero and a standard deviation of `(prod(shape)**-0.5`.\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.scaled_uniform(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return Tensor.uniform(*shape, low=-1.0, high=1.0, **kwargs).mul(prod(argfix(*shape))**-0.5)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.glorot_uniform","title":"glorot_uniform  <code>staticmethod</code>","text":"<pre><code>glorot_uniform(*shape, **kwargs) -&gt; Tensor\n</code></pre> <p>https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.glorot_uniform(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.01166405  0.33367088 -0.21622688]\n [-0.99953824  0.16903277 -1.0515627 ]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef glorot_uniform(*shape, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.glorot_uniform(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  return Tensor.uniform(*shape, low=-1.0, high=1.0, **kwargs).mul((6/(argfix(*shape)[0]+prod(argfix(*shape)[1:])))**0.5)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.kaiming_uniform","title":"kaiming_uniform  <code>staticmethod</code>","text":"<pre><code>kaiming_uniform(\n    *shape, a: float = 0.01, **kwargs\n) -&gt; Tensor\n</code></pre> <p>https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.kaiming_uniform(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[ 0.01505747  0.43074572 -0.27913374]\n [-1.2903337   0.21820946 -1.3574935 ]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef kaiming_uniform(*shape, a:float = 0.01, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.kaiming_uniform(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  bound = math.sqrt(3.0) * math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(argfix(*shape)[1:]))\n  return Tensor.uniform(*shape, low=-bound, high=bound, **kwargs)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.kaiming_normal","title":"kaiming_normal  <code>staticmethod</code>","text":"<pre><code>kaiming_normal(*shape, a: float = 0.01, **kwargs) -&gt; Tensor\n</code></pre> <p>https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_</p> <p>You can pass in <code>dtype</code> and <code>device</code> keyword arguments to control the data type and device of the tensor. Additionally, all other keyword arguments are passed to the constructor of the tensor.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.kaiming_normal(2, 3)\nprint(t.numpy())\n</code></pre> <pre><code>[[-0.65662265 -0.89918077 -0.7425868 ]\n [ 1.0451905  -1.8682908   0.577901  ]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef kaiming_normal(*shape, a:float = 0.01, **kwargs) -&gt; Tensor:\n  \"\"\"\n  &lt;https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_&gt;\n\n  You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.\n  Additionally, all other keyword arguments are passed to the constructor of the tensor.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.kaiming_normal(2, 3)\n  print(t.numpy())\n  ```\n  \"\"\"\n  std = math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(argfix(*shape)[1:]))\n  return Tensor.normal(*shape, mean=0.0, std=std, **kwargs)\n</code></pre>"},{"location":"tensor/#data-access","title":"Data Access","text":""},{"location":"tensor/#tinygrad.Tensor.data","title":"data","text":"<pre><code>data() -&gt; memoryview\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def data(self) -&gt; memoryview:\n  assert self.dtype.fmt is not None, f\"no fmt dtype for {self.dtype}\"\n  assert all_int(self.shape), f\"no data if shape is symbolic, {self.shape=}\"\n  return self._data().cast(self.dtype.fmt, self.shape)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.item","title":"item","text":"<pre><code>item() -&gt; ConstType\n</code></pre> <p>Returns the value of this tensor as a standard Python number.</p> <pre><code>t = Tensor(42)\nprint(t.item())\n</code></pre> <pre><code>42\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def item(self) -&gt; ConstType:\n  \"\"\"\n  Returns the value of this tensor as a standard Python number.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor(42)\n  print(t.item())\n  ```\n  \"\"\"\n  assert self.dtype.fmt is not None, f\"no fmt dtype for {self.dtype}\"\n  assert self.numel() == 1, \"must have one element for item\"\n  return self._data().cast(self.dtype.fmt)[0]\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.tolist","title":"tolist","text":"<pre><code>tolist() -&gt; Union[Sequence[ConstType], ConstType]\n</code></pre> <p>Returns the value of this tensor as a nested list.</p> <pre><code>t = Tensor([1, 2, 3, 4])\nprint(t.tolist())\n</code></pre> <pre><code>[1, 2, 3, 4]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tolist(self) -&gt; Union[Sequence[ConstType], ConstType]:\n  \"\"\"\n  Returns the value of this tensor as a nested list.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(t.tolist())\n  ```\n  \"\"\"\n  return self.data().tolist()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.numpy","title":"numpy","text":"<pre><code>numpy() -&gt; ndarray\n</code></pre> <p>Returns the value of this tensor as a numpy array.</p> <pre><code>t = Tensor([1, 2, 3, 4])\nprint(t.numpy())\n</code></pre> <pre><code>[1 2 3 4]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def numpy(self) -&gt; np.ndarray:\n  \"\"\"\n  Returns the value of this tensor as a numpy array.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3, 4])\n  print(t.numpy())\n  ```\n  \"\"\"\n  if self.dtype == dtypes.bfloat16: return self.float().numpy()\n  assert self.dtype.np is not None, f\"no np dtype for {self.dtype}\"\n  assert all_int(self.shape), f\"no data if shape is symbolic, {self.shape=}\"\n  return np.frombuffer(self._data(), dtype=self.dtype.np).reshape(self.shape)\n</code></pre>"},{"location":"tensor/#movement-low-level","title":"Movement (low level)","text":""},{"location":"tensor/#tinygrad.Tensor.reshape","title":"reshape","text":"<pre><code>reshape(shape, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor with the same data as the original tensor but with a different shape. <code>shape</code> can be passed as a tuple or as separate arguments.</p> <pre><code>t = Tensor.arange(6)\nprint(t.reshape(2, 3).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def reshape(self, shape, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor with the same data as the original tensor but with a different shape.\n  `shape` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6)\n  print(t.reshape(2, 3).numpy())\n  ```\n  \"\"\"\n  new_shape = argfix(shape, *args)\n  new_shape = tuple([-prod(self.shape) // prod(new_shape) if s == -1 else (s if s is not None else self.shape[i]) for i,s in enumerate(new_shape)])\n  return F.Reshape.apply(self, shape=new_shape) if new_shape != self.shape else self\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.expand","title":"expand","text":"<pre><code>expand(shape, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is expanded to the shape that is specified. Expand can also increase the number of dimensions that a tensor has.</p> <p>Passing a <code>-1</code> or <code>None</code> to a dimension means that its size will not be changed.</p> <pre><code>t = Tensor([1, 2, 3])\nprint(t.expand(4, -1).numpy())\n</code></pre> <pre><code>[[1 2 3]\n [1 2 3]\n [1 2 3]\n [1 2 3]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def expand(self, shape, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is expanded to the shape that is specified.\n  Expand can also increase the number of dimensions that a tensor has.\n\n  Passing a `-1` or `None` to a dimension means that its size will not be changed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor([1, 2, 3])\n  print(t.expand(4, -1).numpy())\n  ```\n  \"\"\"\n  return self._broadcast_to(tuple(sh if s==-1 or s is None else s for s, sh in zip(*(_pad_left(argfix(shape, *args), self.shape)))))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.permute","title":"permute","text":"<pre><code>permute(order, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is a permutation of the original tensor. The new tensor has the same data as the original tensor but with the dimensions permuted according to the order specified. <code>order</code> can be passed as a tuple or as separate arguments.</p> <pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.permute(1, 0).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]] -&gt;\n[[0 3]\n [1 4]\n [2 5]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def permute(self, order, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is a permutation of the original tensor.\n  The new tensor has the same data as the original tensor but with the dimensions permuted according to the order specified.\n  `order` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.permute(1, 0).numpy())\n  ```\n  \"\"\"\n  return F.Permute.apply(self, order=argfix(order, *args))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.flip","title":"flip","text":"<pre><code>flip(axis, *args) -&gt; Tensor\n</code></pre> <p>Returns a tensor that reverses the order of the original tensor along given axis. <code>axis</code> can be passed as a tuple or as separate arguments.</p> <pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.flip(0).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]] -&gt;\n[[3 4 5]\n [0 1 2]]\n</code></pre> <pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.flip((0, 1)).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]] -&gt;\n[[5 4 3]\n [2 1 0]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def flip(self, axis, *args) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that reverses the order of the original tensor along given axis.\n  `axis` can be passed as a tuple or as separate arguments.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.flip(0).numpy())\n  ```\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.flip((0, 1)).numpy())\n  ```\n  \"\"\"\n  return F.Flip.apply(self, axis=[x if x &gt;= 0 else x+len(self.shape) for x in argfix(axis, *args)])\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.shrink","title":"shrink","text":"<pre><code>shrink(\n    arg: Tuple[Optional[Tuple[sint, sint]], ...]\n) -&gt; Tensor\n</code></pre> <p>Returns a tensor that shrinks the each axis based on input arg. <code>arg</code> has the same length as <code>self.ndim</code>. For each axis, it can be <code>None</code>, which means no shrink, or a tuple <code>(start, end)</code> that works the same as python slice.</p> <pre><code>t = Tensor.arange(9).reshape(3, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.shrink(((None, (1, 3)))).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]\n [6 7 8]] -&gt;\n[[1 2]\n [4 5]\n [7 8]]\n</code></pre> <pre><code>t = Tensor.arange(9).reshape(3, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.shrink((((0, 2), (0, 2)))).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]\n [6 7 8]] -&gt;\n[[0 1]\n [3 4]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def shrink(self, arg:Tuple[Optional[Tuple[sint, sint]], ...]) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that shrinks the each axis based on input arg.\n  `arg` has the same length as `self.ndim`.\n  For each axis, it can be `None`, which means no shrink, or a tuple `(start, end)` that works the same as python slice.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(3, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.shrink(((None, (1, 3)))).numpy())\n  ```\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(9).reshape(3, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.shrink((((0, 2), (0, 2)))).numpy())\n  ```\n  \"\"\"\n  if all(x is None or x == (0,s) for x,s in zip(arg, self.shape)): return self\n  return F.Shrink.apply(self, arg=tuple(x if x is not None else (0,s) for x,s in zip(arg, self.shape)))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.pad","title":"pad","text":"<pre><code>pad(\n    arg: Tuple[Optional[Tuple[sint, sint]], ...],\n    value: float = 0.0,\n) -&gt; Tensor\n</code></pre> <p>Returns a tensor that pads the each axis based on input arg. arg has the same length as <code>self.ndim</code>. For each axis, it can be <code>None</code>, which means no pad, or a tuple <code>(pad_before, pad_after)</code>. If <code>value</code> is specified, the tensor is padded with <code>value</code>.</p> <pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.pad(((None, (1, 2)))).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]] -&gt;\n[[0 0 1 2 0 0]\n [0 3 4 5 0 0]]\n</code></pre> <pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.pad(((None, (1, 2))), -2).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]] -&gt;\n[[-2  0  1  2 -2 -2]\n [-2  3  4  5 -2 -2]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def pad(self, arg:Tuple[Optional[Tuple[sint, sint]], ...], value:float=0.0) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that pads the each axis based on input arg.\n  arg has the same length as `self.ndim`.\n  For each axis, it can be `None`, which means no pad, or a tuple `(pad_before, pad_after)`.\n  If `value` is specified, the tensor is padded with `value`.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.pad(((None, (1, 2)))).numpy())\n  ```\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.pad(((None, (1, 2))), -2).numpy())\n  ```\n  \"\"\"\n  if all(x is None or x == (0,0) for x in arg): return self\n  ret = F.Pad.apply(self, arg=(narg:=tuple(x if x is not None else (0,0) for x in arg)))\n  return ret if 0 == value else ret + F.Pad.apply(Tensor.ones_like(self), arg=narg).where(0, value)\n</code></pre>"},{"location":"tensor/#movement-high-level","title":"Movement (high level)","text":""},{"location":"tensor/#tinygrad.Tensor.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(indices) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def __getitem__(self, indices) -&gt; Tensor:\n  # 1. indices normalization and validation\n  # treat internal tuples and lists as Tensors and standardize indices to list type\n  if isinstance(indices, list) and all_int(indices): indices = [Tensor(indices, self.device, requires_grad=False)]\n  elif isinstance(indices, (tuple, list)):\n    indices = [Tensor(list(i), self.device, requires_grad=False) if isinstance(i, (tuple, list)) else i for i in indices]\n  else: indices = [indices]\n\n  # turn scalar Tensors into const val for int indexing if possible\n  indices = [self._to_const_val(i) if isinstance(i, Tensor) and i.shape == () else i for i in indices]\n  # move Tensor indices to the same device as self\n  indices = [i.to(self.device) if isinstance(i, Tensor) else i for i in indices]\n\n  # filter ellipsis and fill with slice(None) or fill rest of indices with slice(None)\n  ellipsis_idx = [dim for dim, i in enumerate(indices) if i is Ellipsis]\n  fill_idx = ellipsis_idx[0] if ellipsis_idx else len(indices)\n  num_indices = len(indices) - len(ellipsis_idx) - sum(1 for i in indices if i is None)\n  indices[fill_idx:fill_idx+1] = [slice(None)] * (len(self.shape) - num_indices)\n\n  # use Dict[type, List[dimension]] to track elements in indices\n  type_dim: DefaultDict[Union[type, None], List[int]] = defaultdict(list)\n\n  # record None for dimension injection later and filter None and record rest of indices\n  type_dim[None] = [dim for dim, i in enumerate(indices) if i is None]\n  indices_filtered = [v for v in indices if v is not None]\n  for dim,i in enumerate(indices_filtered): type_dim[type(i)].append(dim)\n\n  for index_type in type_dim:\n    if index_type not in [None, int, slice, Tensor]: raise IndexError(f\"{index_type=} not supported\")\n  if len(ellipsis_idx) &gt; 1: raise IndexError(\"indices can only have a single ellipsis ('...')\")\n  if num_indices &gt; self.ndim: raise IndexError(f\"too many {num_indices=} for {self.ndim=}\")\n\n  # 2. basic indexing, uses only movement ops (no copy)\n  # currently indices_filtered: Tuple[Union[slice, int, Tensor], ...]\n  # turn indices in indices_filtered to Tuple[shrink_arg, strides]\n  for dim in type_dim[int]:\n    if (index := indices_filtered[dim]) &gt;= (size := self.shape[dim]) or index &lt; -size:\n      raise IndexError(f\"{index=} is out of bounds on {dim=} with {size=}\")\n    indices_filtered[dim] = ((index, index+1), 1) if index &gt;= 0 else ((size+index, size+index+1), 1)\n  for dim in type_dim[slice]:\n    if (index := indices_filtered[dim]).step == 0: raise ValueError(f\"{index=} on {dim=} cannot have 0 as step\")\n    s, e, st = index.indices(self.shape[dim])\n    indices_filtered[dim] = ((0, 0) if (st * (e - s)) &lt; 0 else (s, e) if st &gt; 0 else (e+1, s+1), st)\n  # record tensors and skip all Tensor dims for basic indexing\n  tensor_index: List[Tensor] = []\n  for dim in type_dim[Tensor]:\n    tensor_index.append(index := indices_filtered[dim])\n    if not dtypes.is_int(index.dtype): raise IndexError(f\"{index.dtype=} on {dim=} is not supported, only int tensor indexing is supported\")\n    indices_filtered[dim] = ((0, self.shape[dim]), 1)\n\n  new_slice, strides = ((),()) if not indices_filtered else zip(*indices_filtered)\n  ret = self.shrink(new_slice).flip(tuple(i for i, s in enumerate(strides) if s &lt; 0))\n  if any(abs(s) != 1 for s in strides):\n    strides = tuple(abs(s) for s in strides)\n    ret = ret.pad(tuple((0, round_up(sh, s) - sh) for s, sh in zip(strides, ret.shape)))\n    ret = ret.reshape(tuple(flatten((sh // s, s) for s, sh in zip(strides, ret.shape))))\n    ret = ret.shrink(tuple(flatten(((0, sh), (0, 1)) for sh in ret.shape[::2]))).reshape(ret.shape[::2])\n\n  # inject 1 for dim where it's None and collapse dim for int\n  new_shape = list(ret.shape)\n  for dim in type_dim[None]: new_shape.insert(dim, 1)\n  for dim in (dims_collapsed := tuple(dim + sum(1 for d in type_dim[None] if dim &gt;= d) for dim in reversed(type_dim[int]))): new_shape.pop(dim)\n\n  ret = ret.reshape(new_shape)\n  assert all_int(ret.shape), f\"does not support symbolic shape {ret.shape}\"\n\n  # 3. advanced indexing (copy)\n  if type_dim[Tensor]:\n    # calculate dim of current ret by subtracting dims collapsed and adding dims injected up until tensor_dim\n    def calc_dim(tensor_dim:int) -&gt; int:\n      return tensor_dim - sum(1 for d in dims_collapsed if tensor_dim &gt;= d) + sum(1 for d in type_dim[None] if tensor_dim &gt;= d)\n\n    # track tensor_dim and tensor_index using a dict\n    # calc_dim to get dim and use that to normalize the negative tensor indices\n    idx: Dict[int,Tensor] = {(dim := calc_dim(td)):(tensor&lt;0).where(ret.shape[dim],0) + tensor for td,tensor in zip(type_dim[Tensor], tensor_index)}\n\n    masks, first_dim, last_dim = [], min(idx.keys()), max(idx.keys())\n    pre_reduce_shape = ret.shape[:first_dim] + (big_shape := _broadcast_shape(*(t.shape for t in idx.values()))) + ret.shape[first_dim:]\n\n    # create masks\n    for dim, i in idx.items():\n      try: i = i.reshape(i.shape + (1,)*(ret.ndim - first_dim)).expand(pre_reduce_shape)\n      except ValueError as e: raise IndexError(f\"cannot broadcast indices: {e}\") from e\n      a = Tensor.arange(ret.shape[dim], device=self.device, requires_grad=False).reshape((ret.shape[dim],) + (1,)*(ret.ndim - dim - 1))\n      masks.append(i == a)\n\n    # reduce masks to 1 mask\n    mask: Tensor = functools.reduce(lambda x,y: x.mul(y), masks)\n\n    # inject 1's for the extra dims added in create masks\n    sh = ret.shape[:first_dim] + (1,) * len(big_shape) + ret.shape[first_dim:]\n    # sum reduce the extra dims introduced in create masks\n    ret = (ret.reshape(sh) * mask).sum(tuple(i + len(big_shape) for i in idx.keys()), acc_dtype=ret.dtype)\n\n    # special permute case\n    if first_dim != 0 and len(idx) != 1 and tuple(idx.keys()) != tuple(range(first_dim, last_dim+1)):\n      ret = ret.permute(*range(first_dim, first_dim+len(big_shape)), *range(0, first_dim), *range(first_dim+len(big_shape), ret.ndim))\n  return ret\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.slice","title":"slice","text":"<pre><code>slice(\n    arg: Sequence[Optional[Tuple[int, sint]]],\n    value: float = 0,\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def slice(self, arg:Sequence[Optional[Tuple[int, sint]]], value:float=0) -&gt; Tensor:\n  arg_ = tuple(a if a is not None else (0, s) for s,a in zip(self.shape, arg))\n  padding = tuple((max(0, -l), max(0, r-s)) for s,(l,r) in zip(self.shape, arg_))\n  return self.pad(padding, value=value).shrink(tuple((l + pl, r + pl) for (l,r),(pl,_) in zip(arg_, padding)))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.gather","title":"gather","text":"<pre><code>gather(dim: int, index: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def gather(self:Tensor, dim:int, index:Tensor) -&gt; Tensor:\n  assert index.ndim == self.ndim, f\"self.ndim must equal index.ndim, {self.ndim=}, {index.ndim=}\"\n  assert all(s &gt;= i for s,i in zip(self.shape, index.shape)), \"all dim of index.shape must be smaller than self.shape\"\n  dim = self._resolve_dim(dim)\n  index = index.to(self.device).transpose(0, dim).unsqueeze(-1)\n  permarg = list(range(self.ndim))\n  permarg = permarg[1:dim] + [permarg[0]] + permarg[dim+1:] + [permarg[dim]] if dim != 0 else permarg[1:] + [permarg[0]]\n  return ((index == Tensor.arange(self.shape[dim], requires_grad=False, device=self.device)) * self.permute(*permarg).shrink(\n    tuple([*[(0,sh) for sh in index.shape[1:-1]], None])).unsqueeze(0)).sum(-1, acc_dtype=self.dtype).transpose(0, dim)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.cat","title":"cat","text":"<pre><code>cat(*args: Tensor, dim: int = 0) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cat(self:Tensor, *args:Tensor, dim:int=0) -&gt; Tensor:\n  dim = self._resolve_dim(dim)\n  assert all(len(y.shape) == len(self.shape) and all(y.shape[i] == s for i,s in enumerate(self.shape) if i != dim) for y in args)\n  catargs = [self, *args]\n  cat_dims = [s.shape[dim] for s in catargs]\n  cat_dim_cumsum = [0, *itertools.accumulate(cat_dims)]\n  slc:List[List[Optional[Tuple[sint, sint]]]] = [[None for _ in self.shape] for _ in catargs]\n  for d,k,s in zip(cat_dims, cat_dim_cumsum[:-1], slc): s[dim] = (k, cat_dim_cumsum[-1] - k - d)\n  return functools.reduce(Tensor.__add__, [arg.pad(tuple(s)) for arg,s in zip(catargs, slc)])\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.stack","title":"stack  <code>staticmethod</code>","text":"<pre><code>stack(tensors: Sequence[Tensor], dim: int = 0) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef stack(tensors:Sequence[Tensor], dim:int=0) -&gt; Tensor:\n  unsqueezed_tensors = [tensor.unsqueeze(dim) for tensor in tensors]\n  # checks for shapes and number of dimensions delegated to cat\n  return unsqueezed_tensors[0].cat(*unsqueezed_tensors[1:], dim=dim)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.repeat","title":"repeat","text":"<pre><code>repeat(repeats: Sequence[int]) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def repeat(self, repeats:Sequence[int]) -&gt; Tensor:\n  base_shape = (1,) * (len(repeats) - self.ndim) + self.shape\n  new_shape = [x for b in base_shape for x in [1, b]]\n  expand_shape = [x for rs in zip(repeats, base_shape) for x in rs]\n  final_shape = [r*s for r,s in zip(repeats, base_shape)]\n  return self.reshape(new_shape).expand(expand_shape).reshape(final_shape)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.split","title":"split","text":"<pre><code>split(\n    sizes: Union[int, List[int]], dim: int = 0\n) -&gt; Tuple[Tensor, ...]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def split(self, sizes:Union[int, List[int]], dim:int=0) -&gt; Tuple[Tensor, ...]:\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  dim = self._resolve_dim(dim)\n  if isinstance(sizes, int): sizes = [min(sizes, self.shape[dim]-i) for i in range(0, max(1, self.shape[dim]), max(1, sizes))]\n  assert sum(sizes) == self.shape[dim], f\"expect sizes to sum exactly to {self.shape[dim]}, but got {sum(sizes)}\"\n  return tuple(self[sl] for sl in [tuple([slice(None)]*dim + [slice(sum(sizes[:i]), sum(sizes[:i + 1]))]) for i in range(len(sizes))])\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.chunk","title":"chunk","text":"<pre><code>chunk(num: int, dim: int = 0) -&gt; List[Tensor]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def chunk(self, num:int, dim:int=0) -&gt; List[Tensor]:\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  assert num &gt; 0, f\"expect num to be greater than 0, got: {num}\"\n  dim = self._resolve_dim(dim)\n  return list(self.split(math.ceil(self.shape[dim]/num) if self.shape[dim] else [0]*num, dim=dim))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.squeeze","title":"squeeze","text":"<pre><code>squeeze(dim: Optional[int] = None) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def squeeze(self, dim:Optional[int]=None) -&gt; Tensor:\n  if dim is None: return self.reshape(tuple(dim for dim in self.shape if dim != 1))\n  dim = self._resolve_dim(dim)\n  return self if not self.ndim or self.shape[dim] != 1 else self.reshape(self.shape[:dim] + self.shape[dim+1:])\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(dim: int) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def unsqueeze(self, dim:int) -&gt; Tensor:\n  dim = self._resolve_dim(dim, outer=True)\n  return self.reshape(self.shape[:dim] + (1,) + self.shape[dim:])\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.pad2d","title":"pad2d","text":"<pre><code>pad2d(padding: Sequence[int], value: float = 0) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def pad2d(self, padding:Sequence[int], value:float=0) -&gt; Tensor:\n  slc = [(-p0, s+p1) for p0,p1,s in zip(padding[::2], padding[1::2], self.shape[::-1])][::-1]\n  return self.slice([(0,s) for s in self.shape[:-(len(padding)//2)]] + slc, value=value)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.transpose","title":"transpose","text":"<pre><code>transpose(dim0=1, dim1=0) -&gt; Tensor\n</code></pre> <p>Returns a tensor that is a transposed version of the original tensor. The given dimensions <code>dim0</code> and <code>dim1</code> are swapped.</p> <pre><code>t = Tensor.arange(6).reshape(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.transpose(0, 1).numpy())\n</code></pre> <pre><code>[[0 1 2]\n [3 4 5]] -&gt;\n[[0 3]\n [1 4]\n [2 5]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def transpose(self, dim0=1, dim1=0) -&gt; Tensor:\n  \"\"\"\n  Returns a tensor that is a transposed version of the original tensor.\n  The given dimensions `dim0` and `dim1` are swapped.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  t = Tensor.arange(6).reshape(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.transpose(0, 1).numpy())\n  ```\n  \"\"\"\n  order = list(range(self.ndim))\n  order[dim0], order[dim1] = order[dim1], order[dim0]\n  return self.permute(order)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.flatten","title":"flatten","text":"<pre><code>flatten(start_dim=0, end_dim=-1)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def flatten(self, start_dim=0, end_dim=-1):\n  start_dim, end_dim = self._resolve_dim(start_dim), self._resolve_dim(end_dim)\n  return self.reshape(self.shape[:start_dim] + (prod(self.shape[start_dim:end_dim+1]), ) + self.shape[end_dim+1:])\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.unflatten","title":"unflatten","text":"<pre><code>unflatten(dim: int, sizes: Tuple[int, ...])\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def unflatten(self, dim:int, sizes:Tuple[int,...]):\n  dim = self._resolve_dim(dim)\n  return self.reshape(self.shape[:dim] + sizes + self.shape[dim+1:])\n</code></pre>"},{"location":"tensor/#reduce","title":"Reduce","text":""},{"location":"tensor/#tinygrad.Tensor.sum","title":"sum","text":"<pre><code>sum(\n    axis=None,\n    keepdim=False,\n    acc_dtype: Optional[DType] = None,\n)\n</code></pre> <p>Sums the elements of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <p>You can pass in <code>acc_dtype</code> keyword argument to control the data type of the accumulation. If not specified, the accumulation data type is chosen based on the input tensor's data type.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randint(2, 3, low=5, high=10)\nprint(t.numpy(), \"-&gt;\")\nprint(t.sum().numpy())\nprint(t.sum(axis=0).numpy())\nprint(t.sum(axis=1).numpy())\n</code></pre> <pre><code>[[7 8 7]\n [5 7 5]] -&gt;\n39\n[12 15 12]\n[22 17]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sum(self, axis=None, keepdim=False, acc_dtype:Optional[DType]=None):\n  \"\"\"\n  Sums the elements of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  You can pass in `acc_dtype` keyword argument to control the data type of the accumulation.\n  If not specified, the accumulation data type is chosen based on the input tensor's data type.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randint(2, 3, low=5, high=10)\n  print(t.numpy(), \"-&gt;\")\n  print(t.sum().numpy())\n  print(t.sum(axis=0).numpy())\n  print(t.sum(axis=1).numpy())\n  ```\n  \"\"\"\n  ret = self.cast(acc_dtype or sum_acc_dtype(self.dtype))._reduce(F.Sum, axis, keepdim)\n  return ret.cast(self.dtype) if self.dtype in {dtypes.float16, dtypes.bfloat16} else ret\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.max","title":"max","text":"<pre><code>max(axis=None, keepdim=False)\n</code></pre> <p>Returns the maximum value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randint(2, 3, low=5, high=10)\nprint(t.numpy(), \"-&gt;\")\nprint(t.max().numpy())\nprint(t.max(axis=0).numpy())\nprint(t.max(axis=1).numpy())\n</code></pre> <pre><code>[[7 8 7]\n [5 7 5]] -&gt;\n8\n[7 8 7]\n[8 7]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def max(self, axis=None, keepdim=False):\n  \"\"\"\n  Returns the maximum value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randint(2, 3, low=5, high=10)\n  print(t.numpy(), \"-&gt;\")\n  print(t.max().numpy())\n  print(t.max(axis=0).numpy())\n  print(t.max(axis=1).numpy())\n  ```\n  \"\"\"\n  return self._reduce(F.Max, axis, keepdim)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.min","title":"min","text":"<pre><code>min(axis=None, keepdim=False)\n</code></pre> <p>Returns the minimum value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the minimum is computed and whether the reduced dimensions are retained.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randint(2, 3, low=5, high=10)\nprint(t.numpy(), \"-&gt;\")\nprint(t.min().numpy())\nprint(t.min(axis=0).numpy())\nprint(t.min(axis=1).numpy())\n</code></pre> <pre><code>[[7 8 7]\n [5 7 5]] -&gt;\n5\n[5 7 5]\n[7 5]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def min(self, axis=None, keepdim=False):\n  \"\"\"\n  Returns the minimum value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the minimum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randint(2, 3, low=5, high=10)\n  print(t.numpy(), \"-&gt;\")\n  print(t.min().numpy())\n  print(t.min(axis=0).numpy())\n  print(t.min(axis=1).numpy())\n  ```\n  \"\"\"\n  return -((-self).max(axis=axis, keepdim=keepdim))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.mean","title":"mean","text":"<pre><code>mean(axis=None, keepdim=False)\n</code></pre> <p>Returns the mean value of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the mean is computed and whether the reduced dimensions are retained.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randint(2, 3, low=5, high=10)\nprint(t.numpy(), \"-&gt;\")\nprint(t.mean().numpy())\nprint(t.mean(axis=0).numpy())\nprint(t.mean(axis=1).numpy())\n</code></pre> <pre><code>[[7 8 7]\n [5 7 5]] -&gt;\n6.5\n[6.  7.5 6. ]\n[7.3333335 5.666667 ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mean(self, axis=None, keepdim=False):\n  \"\"\"\n  Returns the mean value of the tensor along the specified axis or axes.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the mean is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randint(2, 3, low=5, high=10)\n  print(t.numpy(), \"-&gt;\")\n  print(t.mean().numpy())\n  print(t.mean(axis=0).numpy())\n  print(t.mean(axis=1).numpy())\n  ```\n  \"\"\"\n  output_dtype = self.dtype if dtypes.is_float(self.dtype) else dtypes.float32\n  numerator = self.cast(sum_acc_dtype(self.dtype)).sum(axis=axis, keepdim=keepdim)\n  return numerator.div(prod([si for si, so in zip(self.shape, self.sum(axis=axis, keepdim=True).shape) if si != so])).cast(output_dtype)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.var","title":"var","text":"<pre><code>var(axis=None, keepdim=False, correction=1)\n</code></pre> <p>Returns the variance of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code>, <code>keepdim</code>, and <code>correction</code> keyword arguments to control the axis along which the variance is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randint(2, 3, low=5, high=10)\nprint(t.numpy(), \"-&gt;\")\nprint(t.var().numpy())\nprint(t.var(axis=0).numpy())\nprint(t.var(axis=1).numpy())\n</code></pre> <pre><code>[[7 8 7]\n [5 7 5]] -&gt;\n1.5\n[2.  0.5 2. ]\n[0.33333334 1.3333334 ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def var(self, axis=None, keepdim=False, correction=1):\n  \"\"\"\n  Returns the variance of the tensor along the specified axis or axes.\n\n  You can pass in `axis`, `keepdim`, and `correction` keyword arguments to control the axis along\n  which the variance is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randint(2, 3, low=5, high=10)\n  print(t.numpy(), \"-&gt;\")\n  print(t.var().numpy())\n  print(t.var(axis=0).numpy())\n  print(t.var(axis=1).numpy())\n  ```\n  \"\"\"\n  assert all_int(self.shape), \"does not support symbolic shape\"\n  square_sum = ((self - self.mean(axis=axis, keepdim=True)).square()).sum(axis=axis, keepdim=keepdim)\n  return square_sum.div(max(0, prod(self.shape)/prod(square_sum.shape)-correction))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.std","title":"std","text":"<pre><code>std(axis=None, keepdim=False, correction=1)\n</code></pre> <p>Returns the standard deviation of the tensor along the specified axis or axes.</p> <p>You can pass in <code>axis</code>, <code>keepdim</code>, and <code>correction</code> keyword arguments to control the axis along which the standard deviation is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randint(2, 3, low=5, high=10)\nprint(t.numpy(), \"-&gt;\")\nprint(t.std().numpy())\nprint(t.std(axis=0).numpy())\nprint(t.std(axis=1).numpy())\n</code></pre> <pre><code>[[7 8 7]\n [5 7 5]] -&gt;\n1.2247449\n[1.4142135  0.70710677 1.4142135 ]\n[0.57735026 1.1547005 ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def std(self, axis=None, keepdim=False, correction=1):\n  \"\"\"\n  Returns the standard deviation of the tensor along the specified axis or axes.\n\n  You can pass in `axis`, `keepdim`, and `correction` keyword arguments to control the axis along\n  which the standard deviation is computed, whether the reduced dimensions are retained, and the Bessel's correction applied.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randint(2, 3, low=5, high=10)\n  print(t.numpy(), \"-&gt;\")\n  print(t.std().numpy())\n  print(t.std(axis=0).numpy())\n  print(t.std(axis=1).numpy())\n  ```\n  \"\"\"\n  return self.var(axis, keepdim, correction).sqrt()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.softmax","title":"softmax","text":"<pre><code>softmax(axis=-1)\n</code></pre> <p>Applies the softmax function to the tensor along the specified axis.</p> <p>Rescales the elements of the tensor such that they lie in the range [0, 1] and sum to 1.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the softmax is computed.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.softmax().numpy())\nprint(t.softmax(axis=0).numpy())\n</code></pre> <pre><code>[[-0.8042354  -1.1013222  -0.90952474]\n [ 1.2801557  -2.288294    0.70781666]] -&gt;\n[[0.3783517  0.28110757 0.34054077]\n [0.6279814  0.01770878 0.35430977]]\n[[0.11062321 0.76619905 0.16557185]\n [0.88937676 0.23380095 0.83442813]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softmax(self, axis=-1):\n  \"\"\"\n  Applies the softmax function to the tensor along the specified axis.\n\n  Rescales the elements of the tensor such that they lie in the range [0, 1] and sum to 1.\n\n  You can pass in the `axis` keyword argument to control the axis along which the softmax is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.softmax().numpy())\n  print(t.softmax(axis=0).numpy())\n  ```\n  \"\"\"\n  _, e, ss = self._softmax(axis)\n  return e.div(ss)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.log_softmax","title":"log_softmax","text":"<pre><code>log_softmax(axis=-1)\n</code></pre> <p>Applies the log-softmax function to the tensor along the specified axis.</p> <p>The log-softmax function is a numerically stable alternative to the softmax function in log space.</p> <p>You can pass in the <code>axis</code> keyword argument to control the axis along which the log-softmax is computed.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.log_softmax().numpy())\nprint(t.log_softmax(axis=0).numpy())\n</code></pre> <pre><code>[[-0.8042354  -1.1013222  -0.90952474]\n [ 1.2801557  -2.288294    0.70781666]] -&gt;\n[[-0.97193116 -1.2690179  -1.0772204 ]\n [-0.46524468 -4.0336943  -1.0375837 ]]\n[[-2.2016253  -0.26631325 -1.7983501 ]\n [-0.11723431 -1.4532852  -0.18100864]]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log_softmax(self, axis=-1):\n  \"\"\"\n  Applies the log-softmax function to the tensor along the specified axis.\n\n  The log-softmax function is a numerically stable alternative to the softmax function in log space.\n\n  You can pass in the `axis` keyword argument to control the axis along which the log-softmax is computed.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.log_softmax().numpy())\n  print(t.log_softmax(axis=0).numpy())\n  ```\n  \"\"\"\n  m, _, ss = self._softmax(axis)\n  return m - ss.log()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.logsumexp","title":"logsumexp","text":"<pre><code>logsumexp(axis=None, keepdim=False)\n</code></pre> <p>Computes the log-sum-exp of the tensor along the specified axis or axes.</p> <p>The log-sum-exp function is a numerically stable way to compute the logarithm of the sum of exponentials.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the log-sum-exp is computed and whether the reduced dimensions are retained.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.logsumexp().numpy())\nprint(t.logsumexp(axis=0).numpy())\nprint(t.logsumexp(axis=1).numpy())\n</code></pre> <pre><code>[[-0.8042354  -1.1013222  -0.90952474]\n [ 1.2801557  -2.288294    0.70781666]] -&gt;\n1.9330813\n[ 1.39739   -0.8350089  0.8888253]\n[0.16769573 1.7454003 ]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logsumexp(self, axis=None, keepdim=False):\n  \"\"\"\n  Computes the log-sum-exp of the tensor along the specified axis or axes.\n\n  The log-sum-exp function is a numerically stable way to compute the logarithm of the sum of exponentials.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the log-sum-exp is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.logsumexp().numpy())\n  print(t.logsumexp(axis=0).numpy())\n  print(t.logsumexp(axis=1).numpy())\n  ```\n  \"\"\"\n  m = self.max(axis=axis, keepdim=True)\n  return (self - m).exp().sum(axis=axis, keepdim=keepdim).log() + m.squeeze(axis)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.argmax","title":"argmax","text":"<pre><code>argmax(axis=None, keepdim=False)\n</code></pre> <p>Returns the indices of the maximum value of the tensor along the specified axis.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the maximum is computed and whether the reduced dimensions are retained.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.argmax().numpy()) # Returns the index of the maximum value in the flattened tensor.\nprint(t.argmax(axis=0).numpy()) # Returns the indices of the maximum values along axis 0.\nprint(t.argmax(axis=1).numpy()) # Returns the indices of the maximum values along axis 1.\n</code></pre> <pre><code>[[-0.8042354  -1.1013222  -0.90952474]\n [ 1.2801557  -2.288294    0.70781666]] -&gt;\n3\n[1 0 1]\n[0 0]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def argmax(self, axis=None, keepdim=False):\n  \"\"\"\n  Returns the indices of the maximum value of the tensor along the specified axis.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the maximum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.argmax().numpy()) # Returns the index of the maximum value in the flattened tensor.\n  print(t.argmax(axis=0).numpy()) # Returns the indices of the maximum values along axis 0.\n  print(t.argmax(axis=1).numpy()) # Returns the indices of the maximum values along axis 1.\n  ```\n  \"\"\"\n  if axis is None:\n    idx = (self == self.max(axis)) * Tensor.arange(prod(self.shape)-1,-1,-1, requires_grad=False, device=self.device).reshape(self.shape)\n    return (prod(self.shape) - idx.max() - 1).cast(dtypes.int32)\n  axis = self._resolve_dim(axis)\n  m = self == self.max(axis=axis, keepdim=True)\n  idx = m * Tensor.arange(self.shape[axis]-1,-1,-1, requires_grad=False, device=self.device).reshape(self.shape[axis], *[1]*(self.ndim-axis-1))\n  return (self.shape[axis]-idx.max(axis=axis, keepdim=keepdim)-1).cast(dtypes.int32)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.argmin","title":"argmin","text":"<pre><code>argmin(axis=None, keepdim=False)\n</code></pre> <p>Returns the indices of the minimum value of the tensor along the specified axis.</p> <p>You can pass in <code>axis</code> and <code>keepdim</code> keyword arguments to control the axis along which the minimum is computed and whether the reduced dimensions are retained.</p> <pre><code>Tensor.manual_seed(42)\nt = Tensor.randn(2, 3)\nprint(t.numpy(), \"-&gt;\")\nprint(t.argmin().numpy()) # Returns the index of the minimum value in the flattened tensor.\nprint(t.argmin(axis=0).numpy()) # Returns the indices of the minimum values along axis 0.\nprint(t.argmin(axis=1).numpy()) # Returns the indices of the minimum values along axis 1.\n</code></pre> <pre><code>[[-0.8042354  -1.1013222  -0.90952474]\n [ 1.2801557  -2.288294    0.70781666]] -&gt;\n4\n[0 1 0]\n[1 1]\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def argmin(self, axis=None, keepdim=False):\n  \"\"\"\n  Returns the indices of the minimum value of the tensor along the specified axis.\n\n  You can pass in `axis` and `keepdim` keyword arguments to control the axis along\n  which the minimum is computed and whether the reduced dimensions are retained.\n\n  ```python exec=\"true\" source=\"above\" session=\"tensor\" result=\"python\"\n  Tensor.manual_seed(42)\n  t = Tensor.randn(2, 3)\n  print(t.numpy(), \"-&gt;\")\n  print(t.argmin().numpy()) # Returns the index of the minimum value in the flattened tensor.\n  print(t.argmin(axis=0).numpy()) # Returns the indices of the minimum values along axis 0.\n  print(t.argmin(axis=1).numpy()) # Returns the indices of the minimum values along axis 1.\n  ```\n  \"\"\"\n  return (-self).argmax(axis=axis, keepdim=keepdim)\n</code></pre>"},{"location":"tensor/#processing","title":"Processing","text":""},{"location":"tensor/#tinygrad.Tensor.conv2d","title":"conv2d","text":"<pre><code>conv2d(\n    weight: Tensor,\n    bias: Optional[Tensor] = None,\n    groups=1,\n    stride=1,\n    dilation=1,\n    padding=0,\n    acc_dtype: Optional[DType] = None,\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def conv2d(self, weight:Tensor, bias:Optional[Tensor]=None, groups=1, stride=1, dilation=1, padding=0, acc_dtype:Optional[DType]=None) -&gt; Tensor:\n  (bs,cin_), (cout,cin), HW = self.shape[:2], weight.shape[:2], weight.shape[2:]\n  assert groups*cin == cin_ and len(self.shape) == len(weight.shape), f\"Input Tensor shape {self.shape} does not match the shape of the weights {weight.shape}. ({groups*cin} vs. {cin_})\"  # noqa: E501\n  if isinstance(padding, (tuple,list)): assert len(padding) == 2*len(HW) or len(padding) == len(HW), f\"Expected padding of length {2*len(HW)} or {len(HW)}, but got {len(padding)} for tensor of shape {self.shape}\"  # noqa: E501\n  padding_ = [padding]*2*len(HW) if isinstance(padding, int) else (padding if len(padding) == 2*len(HW) else [p for p in padding for _ in range(2)][::-1])  # noqa: E501\n\n  # conv2d is a pooling op (with padding)\n  x = self.pad2d(padding_)._pool(HW, stride, dilation)   # (bs, groups*cin, oy, ox, H, W)\n  rcout, oyx = cout//groups, x.shape[2:-len(HW)]\n  if not all(x == 3 for x in HW) or stride != 1 or dilation != 1 or not WINO:\n    # normal conv\n    x = x.reshape(bs, groups, cin, 1, *oyx, *HW).expand(bs, groups, cin, rcout, *oyx, *HW).permute(0,1,3,*[4+i for i in range(len(oyx))],2,*[4+len(oyx)+i for i in range(len(HW))])  # noqa: E501\n\n    # conv! broadcasted to (bs, groups, rcout, *oyx, cin, *HW)\n    ret = (x * weight.reshape(1, groups, rcout, *[1] * len(oyx), cin, *HW)).sum([-1-i for i in range(1+len(oyx))], keepdim=True, acc_dtype=acc_dtype).reshape(bs, cout, *oyx)  # noqa: E501\n    return ret if bias is None else ret.add(bias.reshape(1, -1, *[1] * len(HW)))\n\n  HWI, HWO = (6,) * len(HW), (4,) * len(HW)  # F(4x4,3x3) winograd tiles\n  winograd_G = [[1/4, 0, 0], [-1/6, -1/6, -1/6], [-1/6, 1/6, -1/6], [1/24, 1/12, 1/6], [1/24, -1/12, 1/6], [0, 0, 1]]\n  winograd_Bt = [[4, 0, -5, 0, 1, 0], [0, -4, -4, 1, 1, 0], [0, 4, -4, -1, 1, 0], [0, -2, -1, 2, 1, 0], [0, 2, -1, -2, 1, 0], [0, 4, 0, -5, 0, 1]]\n  winograd_At = [[1, 1, 1, 1, 1, 0], [0, 1, -1, 2, -2, 0], [0, 1, 1, 4, 4, 0], [0, 1, -1, 8, -8, 1]] # applying At in pre-order doubles compile time\n\n  # todo: stride == dilation\n  # use padding to round up to 4x4 output tiles\n  # (bs, cin_, tyx, HWI)\n  d = self.pad2d(sum([[padding_[i*2], padding_[i*2+1] + (-(dim + sum(padding_[i * 2:(i + 1) * 2]) - 2) % 4)] for i, dim in enumerate(self.shape[-len(HW):])], []))._pool(HWI, HWO)  # noqa: E501\n  # move HW to the front: # (HWI, bs, cin_, tyx)\n  d = d.permute(*range(len(d.shape)-len(HW),len(d.shape)), *range(len(d.shape)-len(HW)))\n  tyx = d.shape[-len(HWI):]  # dim of tiling\n\n  g = weight.permute(*range(len(weight.shape)-len(HW),len(weight.shape)), *range(len(weight.shape)-len(HW)))  # move HW to the front\n\n  # compute 6x6 winograd tiles: GgGt, BtdB\n  # (HWI, groups * rcout, cin) -&gt; (HWI, bs=1, groups, rcout, cin, tyx=(1,1))\n  gfactors = _apply_winograd_matrix(winograd_G, g, len(HW)).reshape(*HWI, 1, groups, rcout, cin, *([1]*len(tyx)))\n  # (HWI, bs, cin_, tyx) -&gt; (HWI, bs, groups, 1 ,cin, *tyx)\n  dfactors = _apply_winograd_matrix(winograd_Bt, d, len(HW)).reshape(*HWI, bs, groups, 1, cin, *tyx)\n\n  # matmul; sum across cin: (HWI, bs, groups, rcout, *tyx); then HWI -&gt; HWO: (HWO, bs, groups, rcout, *tyx)\n  ret = _apply_winograd_matrix(winograd_At, (gfactors * dfactors).sum(axis=-1-len(HW), acc_dtype=acc_dtype), len(HW))\n\n  # interleave tyx and HWO: (bs, groups, rcout, oy, HO, ox, WO)\n  ret = ret.permute([*range(len(HW), len(ret.shape)-len(HW)), *[i+o for i in range(len(HW)) for o in [len(ret.shape)-len(HW),0]]])\n  # merge groups and rcout, tyx and HWO: (bs, groups, cout, *yx), shrink to final\n  ret = ret.reshape(bs, cout, *[c * HWO[i] for i, c in enumerate(tyx)]).shrink(tuple((0, s) for s in [bs, cout, *oyx]))\n\n  return (ret if bias is None else ret.add(bias.reshape(1, -1, *[1 for _ in range(len(HW))]))).contiguous().contiguous_backward()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.dot","title":"dot","text":"<pre><code>dot(w: Tensor, acc_dtype: Optional[DType] = None) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def dot(self, w:Tensor, acc_dtype:Optional[DType]=None) -&gt; Tensor:\n  n1, n2 = len(self.shape), len(w.shape)\n  assert n1 != 0 and n2 != 0, f\"both arguments to matmul need to be at least 1D, but they are {n1}D and {n2}D\"\n  assert (L:=self.shape[-1]) == (R:=w.shape[-min(n2, 2)]), f\"Input Tensor shapes {self.shape} and {w.shape} cannot be multiplied ({L} != {R})\"\n  x = self.reshape(*self.shape[0:-1], *[1]*min(n1-1, n2-1, 1), self.shape[-1])\n  w = w.reshape(*w.shape[0:-2], *[1]*min(n1-1, n2-1, 1), *w.shape[-min(n2, 2):]).transpose(-1, -min(n2, 2))\n  return (x*w).sum(-1, acc_dtype=acc_dtype).cast(least_upper_dtype(x.dtype, w.dtype))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.matmul","title":"matmul","text":"<pre><code>matmul(\n    x: Tensor,\n    reverse=False,\n    acc_dtype: Optional[DType] = None,\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def matmul(self, x:Tensor, reverse=False, acc_dtype:Optional[DType]=None) -&gt; Tensor:\n  return x.dot(self, acc_dtype=acc_dtype) if reverse else self.dot(x, acc_dtype=acc_dtype)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.einsum","title":"einsum  <code>staticmethod</code>","text":"<pre><code>einsum(\n    formula: str, *raw_xs, acc_dtype: Optional[DType] = None\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>@staticmethod\ndef einsum(formula:str, *raw_xs, acc_dtype:Optional[DType]=None) -&gt; Tensor:\n  xs:Tuple[Tensor] = argfix(*raw_xs)\n  formula = formula.replace(\" \", \"\")\n  inputs_str, output = formula.split(\"-&gt;\") if \"-&gt;\" in formula else (formula, sorted(formula))\n  inputs = [x for x in cast(str,inputs_str).split(',')]\n  assert len(xs) == len(inputs), f\"number of inputs doesn't match number of operands in formula, expected {len(inputs)}, got {len(xs)}\"\n\n  # map the value of each letter in the formula\n  letter_val = sorted(merge_dicts([{letter:dim for letter, dim in zip(letters, tensor.shape)} for letters, tensor in zip(inputs, xs)]).items())\n\n  xs_:List[Tensor] = []\n  lhs = [sorted(enumerate(s), key=lambda e:e[1]) for s in inputs]\n  for x,(order,letters) in zip(xs, [list(zip(*l)) for l in lhs]):\n    # permute to the sorted letter order, then reshape/expand to create dimensions for the missing letters\n    xs_.append(x.permute(order).reshape([val if letter in letters else 1 for letter,val in letter_val]).expand([val for _,val in letter_val]))\n\n  # determine the inverse permutation to revert back to original order\n  rhs_letter_order = argsort(list(output))\n  rhs_order = argsort(rhs_letter_order)\n\n  # sum over all axes that's not in the output, then permute to the output order\n  return functools.reduce(lambda a,b:a*b, xs_) \\\n    .sum(axis=[axis for axis,(letter,_) in enumerate(letter_val) if letter not in output],acc_dtype=acc_dtype).permute(rhs_order)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.cumsum","title":"cumsum","text":"<pre><code>cumsum(axis: int = 0) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cumsum(self, axis:int=0) -&gt; Tensor:\n  # TODO: someday the optimizer will find this on it's own\n  # for now this is a two stage cumsum\n  SPLIT = 256\n  if self.shape[axis] &lt;= SPLIT*2: return self._cumsum(axis)\n  ret = self.transpose(axis,-1).pad2d((round_up(self.shape[axis], SPLIT)-self.shape[axis], 0))\n  ret = ret.unflatten(-1, (-1, SPLIT))._cumsum(-1)\n  base_add = ret[..., -1]._cumsum(-1, _first_zero=True)[..., :-1]\n  base_add = base_add.unsqueeze(-1).expand(*base_add.shape, ret.shape[-1])\n  def fix(x:Tensor): return x.flatten(start_dim=-2)[..., -self.shape[axis]:].transpose(axis,-1)\n  return fix(ret) + fix(base_add)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.triu","title":"triu","text":"<pre><code>triu(k: int = 0) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def triu(self, k:int=0) -&gt; Tensor: return Tensor._tri(self.shape[-2], self.shape[-1], k=k, device=self.device).where(self, 0)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.tril","title":"tril","text":"<pre><code>tril(k: int = 0) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tril(self, k:int=0) -&gt; Tensor: return Tensor._tri(self.shape[-2], self.shape[-1], k=k+1, device=self.device).where(0, self)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.avg_pool2d","title":"avg_pool2d","text":"<pre><code>avg_pool2d(kernel_size=(2, 2), stride=None, dilation=1)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def avg_pool2d(self, kernel_size=(2,2), stride=None, dilation=1): return self._pool(\n      make_pair(kernel_size), stride if stride is not None else kernel_size, dilation).mean(axis=tuple(range(0-len(make_pair(kernel_size)), 0)))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.max_pool2d","title":"max_pool2d","text":"<pre><code>max_pool2d(kernel_size=(2, 2), stride=None, dilation=1)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def max_pool2d(self, kernel_size=(2,2), stride=None, dilation=1): return self._pool(\n      make_pair(kernel_size), stride if stride is not None else kernel_size, dilation).max(axis=tuple(range(0-len(make_pair(kernel_size)), 0)))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.conv_transpose2d","title":"conv_transpose2d","text":"<pre><code>conv_transpose2d(\n    weight: Tensor,\n    bias: Optional[Tensor] = None,\n    groups=1,\n    stride=1,\n    dilation=1,\n    padding=0,\n    output_padding=0,\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def conv_transpose2d(self, weight:Tensor, bias:Optional[Tensor]=None, groups=1, stride=1, dilation=1, padding=0, output_padding=0) -&gt; Tensor:\n  HW, trailing = weight.shape[2:], list(range(3, len(weight.shape)+1))\n  x, w = self, weight.unflatten(0, (groups, -1)).permute(0,2,1,*trailing).flip(trailing)\n  stride = make_pair(stride, len(HW))\n  if any(s&gt;1 for s in stride):\n    x = x.reshape(None, None, *flatten((k,1) for k in x.shape[2:]))\n    x = x.pad((None, None, *flatten((None,(0,s-1)) for s in stride)))\n    x = x.reshape(None, None, *[k*s for k,s in zip(x.shape[2::2], stride)])\n    x = x.shrink((None, None, *[(0,k-(s-1)) for k,s in zip(x.shape[2:], stride)]))\n  padding = flatten((((k-1)*d-p,(k-1)*d-p+op) for k,d,p,op in reversed(list(\n    zip(HW, make_pair(dilation, len(HW)), make_pair(padding, len(HW)), make_pair(output_padding, len(HW)))))))\n  return x.conv2d(w.flatten(end_dim=1), groups=groups, bias=bias, dilation=dilation, padding=padding)\n</code></pre>"},{"location":"tensor/#unary-ops-math","title":"Unary Ops (math)","text":""},{"location":"tensor/#tinygrad.Tensor.logical_not","title":"logical_not","text":"<pre><code>logical_not()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def logical_not(self): return F.Eq.apply(*self._broadcasted(False))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.neg","title":"neg","text":"<pre><code>neg()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def neg(self): return F.Neg.apply(self) if self.dtype != dtypes.bool else self.logical_not()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.log","title":"log","text":"<pre><code>log()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log(self): return F.Log.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.log2","title":"log2","text":"<pre><code>log2()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def log2(self): return self.log()/math.log(2)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.exp","title":"exp","text":"<pre><code>exp()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def exp(self): return F.Exp.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.exp2","title":"exp2","text":"<pre><code>exp2()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def exp2(self): return F.Exp.apply(self*math.log(2))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.trunc","title":"trunc","text":"<pre><code>trunc() -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def trunc(self: Tensor) -&gt; Tensor: return self.cast(dtypes.int32).cast(self.dtype)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.ceil","title":"ceil","text":"<pre><code>ceil() -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def ceil(self: Tensor) -&gt; Tensor: return (self &gt; (b := self.trunc())).where(b+1, b)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.floor","title":"floor","text":"<pre><code>floor() -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def floor(self: Tensor) -&gt; Tensor: return (self &lt; (b := self.trunc())).where(b-1, b)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.round","title":"round","text":"<pre><code>round() -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def round(self: Tensor) -&gt; Tensor:\n  return ((self &gt; 0) == ((b := self.cast(dtypes.int32) / 2.0).cast(dtypes.int32) == b)).where((self - 0.5).ceil(), (self + 0.5).floor())\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.lerp","title":"lerp","text":"<pre><code>lerp(end: Tensor, weight: Union[Tensor, float]) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def lerp(self, end: Tensor, weight: Union[Tensor, float]) -&gt; Tensor: return self + (end - self) * weight\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.square","title":"square","text":"<pre><code>square()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def square(self): return self*self\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.clip","title":"clip","text":"<pre><code>clip(min_, max_)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def clip(self, min_, max_): return self.maximum(min_).minimum(max_)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.abs","title":"abs","text":"<pre><code>abs()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def abs(self): return self * self.sign()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.sign","title":"sign","text":"<pre><code>sign()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sign(self): return F.Sign.apply(self)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.reciprocal","title":"reciprocal","text":"<pre><code>reciprocal()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def reciprocal(self): return F.Reciprocal.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/#unary-ops-activation","title":"Unary Ops (activation)","text":""},{"location":"tensor/#tinygrad.Tensor.relu","title":"relu","text":"<pre><code>relu()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def relu(self): return F.Relu.apply(self)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.sigmoid","title":"sigmoid","text":"<pre><code>sigmoid()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sigmoid(self): return F.Sigmoid.apply(self.cast(least_upper_float(self.dtype)))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.elu","title":"elu","text":"<pre><code>elu(alpha=1.0)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def elu(self, alpha=1.0): return self.relu() - alpha*(1-self.exp()).relu()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.celu","title":"celu","text":"<pre><code>celu(alpha=1.0)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def celu(self, alpha=1.0): return self.maximum(0) + (alpha * ((self / alpha).exp() - 1)).minimum(0)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.swish","title":"swish","text":"<pre><code>swish()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def swish(self): return self * self.sigmoid()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.silu","title":"silu","text":"<pre><code>silu()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def silu(self): return self.swish()   # The SiLU function is also known as the swish function.\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.relu6","title":"relu6","text":"<pre><code>relu6()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def relu6(self): return self.relu() - (self-6).relu()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.hardswish","title":"hardswish","text":"<pre><code>hardswish()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def hardswish(self): return self * (self+3).relu6() * (1/6)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.tanh","title":"tanh","text":"<pre><code>tanh()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def tanh(self): return 2.0 * ((2.0 * self).sigmoid()) - 1.0\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.sinh","title":"sinh","text":"<pre><code>sinh()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sinh(self): return (self.exp() - self.neg().exp()) / 2\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.cosh","title":"cosh","text":"<pre><code>cosh()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cosh(self): return (self.exp() + self.neg().exp()) / 2\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.atanh","title":"atanh","text":"<pre><code>atanh()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def atanh(self): return ((1 + self)/(1 - self)).log() / 2\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.asinh","title":"asinh","text":"<pre><code>asinh()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def asinh(self): return (self + (self.square() + 1).sqrt()).log()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.acosh","title":"acosh","text":"<pre><code>acosh()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def acosh(self): return (self + (self.square() - 1).sqrt()).log()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.hardtanh","title":"hardtanh","text":"<pre><code>hardtanh(min_val=-1, max_val=1)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def hardtanh(self, min_val=-1, max_val=1): return self.clip(min_val, max_val)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.gelu","title":"gelu","text":"<pre><code>gelu()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def gelu(self): return 0.5 * self * (1 + (self * 0.7978845608 * (1 + 0.044715 * self * self)).tanh())\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.quick_gelu","title":"quick_gelu","text":"<pre><code>quick_gelu()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def quick_gelu(self): return self * (self * 1.702).sigmoid()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.leakyrelu","title":"leakyrelu","text":"<pre><code>leakyrelu(neg_slope=0.01)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def leakyrelu(self, neg_slope=0.01): return self.relu() - (-neg_slope*self).relu()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.mish","title":"mish","text":"<pre><code>mish()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mish(self): return self * self.softplus().tanh()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.softplus","title":"softplus","text":"<pre><code>softplus(beta=1)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softplus(self, beta=1): return (1/beta) * (1 + (self*beta).exp()).log()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.softsign","title":"softsign","text":"<pre><code>softsign()\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def softsign(self): return self / (1 + self.abs())\n</code></pre>"},{"location":"tensor/#elementwise-ops-broadcasted","title":"Elementwise Ops (broadcasted)","text":""},{"location":"tensor/#tinygrad.Tensor.add","title":"add","text":"<pre><code>add(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def add(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor: return F.Add.apply(*self._broadcasted(x, reverse))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.sub","title":"sub","text":"<pre><code>sub(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sub(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor: return F.Sub.apply(*self._broadcasted(x, reverse))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.mul","title":"mul","text":"<pre><code>mul(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def mul(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor: return F.Mul.apply(*self._broadcasted(x, reverse))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.div","title":"div","text":"<pre><code>div(\n    x: Union[Tensor, ConstType], reverse=False, upcast=True\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def div(self, x:Union[Tensor, ConstType], reverse=False, upcast=True) -&gt; Tensor:\n  numerator, denominator = self._broadcasted(x, reverse)\n  if upcast: numerator, denominator = numerator.cast(least_upper_float(numerator.dtype)), denominator.cast(least_upper_float(denominator.dtype))\n  return F.Div.apply(numerator, denominator)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.xor","title":"xor","text":"<pre><code>xor(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def xor(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor: return F.Xor.apply(*self._broadcasted(x, reverse))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.pow","title":"pow","text":"<pre><code>pow(x: Union[Tensor, ConstType], reverse=False) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def pow(self, x:Union[Tensor, ConstType], reverse=False) -&gt; Tensor:\n  x = self._to_const_val(x)\n  if not isinstance(x, Tensor) and not reverse:\n    # simple pow identities\n    if x &lt; 0: return self.reciprocal().pow(-x)\n    if x == 0: return 1 + self * 0\n    if x in [3,2,1]: return functools.reduce(lambda acc,_: acc * self, range(int(x)-1), self)\n    if x == 0.5: return self.sqrt()\n  if not isinstance(x, Tensor) and reverse and x &gt; 0: return self.mul(math.log(x)).exp()\n  ar = self.abs().log().mul(x).exp() if not reverse or isinstance(x, Tensor) else self.mul(math.log(abs(x))).exp()\n  # correct sign of negative numbers raised to a power (cos has a period of 2pi so we use it here to get the oddness of the power)\n  sign = (x * math.pi).cos() if isinstance(x, Tensor) else math.cos(x * math.pi) if not reverse else (self * math.pi).cos()\n  # we only need to correct the sign if the base is negative\n  base_sign = ((self.sign() if not reverse else x.sign() if isinstance(x, Tensor) else math.copysign(1, x)) - 1) / -2\n  # we need 0 to be positive so we need to correct base_sign when the base is 0\n  base_sign = base_sign - (1.5 * (1 - (self.sign().abs() if not reverse else x.sign().abs() if isinstance(x, Tensor) else abs(int(bool(x))))))\n  # inject nan if the base is negative and the power is not an integer\n  to_nan = (((x - x.trunc()) * 1e10).abs().clip(0, 1) if isinstance(x, Tensor) else \\\n            int(bool(x - int(x))) if not reverse else ((self - self.trunc()) * 1e10).abs().clip(0, 1)) * base_sign\n  inject_nan = ((((-to_nan) * 2) + 1)).log().add(1) if isinstance(to_nan, Tensor) else 1 if not to_nan else float(\"nan\")\n  return ar.mul(sign * base_sign + (1 - base_sign)).mul(inject_nan)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.maximum","title":"maximum","text":"<pre><code>maximum(x: Union[Tensor, ConstType]) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def maximum(self, x:Union[Tensor, ConstType]) -&gt; Tensor:\n  return (self&lt;x).detach().where(x, (self==x).detach().where(((self * 0.5 + x * 0.5).cast(self.dtype)), self))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.minimum","title":"minimum","text":"<pre><code>minimum(x: Union[Tensor, ConstType]) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def minimum(self, x:Union[Tensor, ConstType]) -&gt; Tensor: return -((-self).maximum(-x))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.where","title":"where","text":"<pre><code>where(\n    input_: Union[Tensor, ConstType],\n    other: Union[Tensor, ConstType],\n)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def where(self:Tensor, input_:Union[Tensor, ConstType], other:Union[Tensor, ConstType]):\n  if isinstance(input_, Tensor): input_, other = input_._broadcasted(other)\n  elif isinstance(other, Tensor): other, input_ = other._broadcasted(input_)\n  x_,y = self._broadcasted(input_, match_dtype=False)\n  x,z = x_._broadcasted(other, match_dtype=False)\n  return F.Where.apply(x.cast(dtypes.bool), *y._broadcasted(z))\n</code></pre>"},{"location":"tensor/#neural-network-ops-functional","title":"Neural Network Ops (functional)","text":""},{"location":"tensor/#tinygrad.Tensor.linear","title":"linear","text":"<pre><code>linear(weight: Tensor, bias: Optional[Tensor] = None)\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def linear(self, weight:Tensor, bias:Optional[Tensor]=None):\n  x = self.mul(weight) if len(weight.shape) == 1 else self.dot(weight)\n  return x.add(bias) if bias is not None else x\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.sequential","title":"sequential","text":"<pre><code>sequential(ll: List[Callable[[Tensor], Tensor]])\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sequential(self, ll:List[Callable[[Tensor], Tensor]]): return functools.reduce(lambda x,f: f(x), ll, self)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.layernorm","title":"layernorm","text":"<pre><code>layernorm(axis=-1, eps: float = 1e-05) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def layernorm(self, axis=-1, eps:float=1e-5) -&gt; Tensor:\n  y = (self - self.mean(axis, keepdim=True))\n  return y.mul((y*y).mean(axis, keepdim=True).add(eps).rsqrt())\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.batchnorm","title":"batchnorm","text":"<pre><code>batchnorm(\n    weight: Optional[Tensor],\n    bias: Optional[Tensor],\n    mean: Tensor,\n    invstd: Tensor,\n    axis: Union[int, Tuple[int, ...]] = 1,\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def batchnorm(self, weight:Optional[Tensor], bias:Optional[Tensor], mean:Tensor, invstd:Tensor, axis:Union[int,Tuple[int,...]]=1) -&gt; Tensor:\n  axis_ = argfix(axis)\n  shape = tuple(s if ax in axis_ else 1 for ax, s in enumerate(self.shape))\n  x = self - mean.reshape(shape)\n  if weight is not None: x = x * weight.reshape(shape)\n  ret = x.mul(invstd.reshape(shape) if len(invstd.shape) == len(axis_) else invstd)\n  return (ret + bias.reshape(shape)) if bias is not None else ret\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.dropout","title":"dropout","text":"<pre><code>dropout(p=0.5) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def dropout(self, p=0.5) -&gt; Tensor:\n  if not Tensor.training or p == 0: return self\n  return self * (Tensor.rand(*self.shape, requires_grad=False, dtype=dtypes.default_float, device=self.device) &gt;= p) * (1/(1.0 - p))\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.one_hot","title":"one_hot","text":"<pre><code>one_hot(num_classes: int) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def one_hot(self, num_classes:int) -&gt; Tensor:\n  return (self[..., None] == Tensor.arange(num_classes, requires_grad=False, device=self.device)).where(1, 0)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.scaled_dot_product_attention","title":"scaled_dot_product_attention","text":"<pre><code>scaled_dot_product_attention(\n    key: Tensor,\n    value: Tensor,\n    attn_mask: Optional[Tensor] = None,\n    dropout_p: float = 0.0,\n    is_causal: bool = False,\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def scaled_dot_product_attention(self, key:Tensor, value:Tensor, attn_mask:Optional[Tensor]=None,\n                                 dropout_p:float=0.0, is_causal:bool=False) -&gt; Tensor:\n  # NOTE: it works if key, value have symbolic shape\n  assert all_int(self.shape), f\"does not support symbolic shape {self.shape}\"\n  if is_causal: attn_mask = Tensor.ones(self.shape[-2], key.shape[-2], requires_grad=False, device=self.device).tril(0).cast(dtypes.bool)\n  if attn_mask is not None and attn_mask.dtype == dtypes.bool: attn_mask = (attn_mask == 0).where(-float(\"inf\"), 0)\n  qk = self @ key.transpose(-2,-1) / math.sqrt(self.shape[-1])\n  return ((qk+attn_mask) if attn_mask is not None else qk).softmax(-1).dropout(dropout_p) @ value\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.binary_crossentropy","title":"binary_crossentropy","text":"<pre><code>binary_crossentropy(y: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def binary_crossentropy(self, y:Tensor) -&gt; Tensor:\n  return (-y*self.log() - (1-y)*(1-self).log()).mean()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.binary_crossentropy_logits","title":"binary_crossentropy_logits","text":"<pre><code>binary_crossentropy_logits(y: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def binary_crossentropy_logits(self, y:Tensor) -&gt; Tensor:\n  return (self.maximum(0) - y * self + (1 + self.abs().neg().exp()).log()).mean()\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.sparse_categorical_crossentropy","title":"sparse_categorical_crossentropy","text":"<pre><code>sparse_categorical_crossentropy(\n    Y: Tensor, ignore_index=-1, label_smoothing=0.0\n) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def sparse_categorical_crossentropy(self, Y:Tensor, ignore_index=-1, label_smoothing=0.0) -&gt; Tensor:\n  assert 0.0 &lt;= label_smoothing &lt;= 1.0, \"label_smoothing must be in [0.0, 1.0]\"\n  # NOTE: self is a logits input\n  log_probs, loss_mask = self.log_softmax(), (Y != ignore_index)\n  y_counter = Tensor.arange(self.shape[-1], requires_grad=False, device=self.device).unsqueeze(0).expand(Y.numel(), self.shape[-1])\n  y = ((y_counter == Y.flatten().reshape(-1, 1)) * loss_mask.reshape(-1, 1)).reshape(*Y.shape, self.shape[-1])\n  smoothing = label_smoothing * (log_probs.mean(-1) * loss_mask).sum()\n  return -((1 - label_smoothing) * (log_probs * y).sum() + smoothing) / loss_mask.sum()\n</code></pre>"},{"location":"tensor/#casting-ops","title":"Casting Ops","text":""},{"location":"tensor/#tinygrad.Tensor.cast","title":"cast","text":"<pre><code>cast(dtype: DType) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def cast(self, dtype:DType) -&gt; Tensor: return self if self.dtype == dtype else F.Cast.apply(self, dtype=dtype)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.bitcast","title":"bitcast","text":"<pre><code>bitcast(dtype: DType) -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def bitcast(self, dtype:DType) -&gt; Tensor:\n  if self.requires_grad: raise RuntimeError(\"can't backprop through bitcast\")\n  return F.Cast.apply(self, dtype=dtype, bitcast=True) if self.dtype != dtype else self\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.float","title":"float","text":"<pre><code>float() -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def float(self) -&gt; Tensor: return self.cast(dtypes.float32)\n</code></pre>"},{"location":"tensor/#tinygrad.Tensor.half","title":"half","text":"<pre><code>half() -&gt; Tensor\n</code></pre> Source code in <code>tinygrad/tensor.py</code> <pre><code>def half(self) -&gt; Tensor: return self.cast(dtypes.float16)\n</code></pre>"}]}